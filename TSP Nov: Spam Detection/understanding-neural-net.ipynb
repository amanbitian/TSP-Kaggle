{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3d28df",
   "metadata": {
    "papermill": {
     "duration": 0.017238,
     "end_time": "2021-11-02T06:19:53.245760",
     "exception": false,
     "start_time": "2021-11-02T06:19:53.228522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **<font color='red'>NN BASELINE</font>**\n",
    "**<font color='blue'>In this notebook i have implemented a baseline model of Neural Network (do let me know if you have any doubts or find any bugs)</font>**\n",
    "\n",
    "\n",
    "* **SCOPE OF IMPROVEMENTS**\n",
    "1. **i haven't tested effect of batch size**\n",
    "2. **adding layers**\n",
    "3. **feature engineering on combined train and test data.**\n",
    "4. **analize train and test distribution carefully.**\n",
    "5. **try different optimizers (Nadam,AdamW,Adagrad etc.)**\n",
    "6. **try different scheduling techniques.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1676e84c",
   "metadata": {
    "papermill": {
     "duration": 0.014485,
     "end_time": "2021-11-02T06:19:53.276772",
     "exception": false,
     "start_time": "2021-11-02T06:19:53.262287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **<font color='green'>IMPORT LIBRARIES</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2631c562",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-02T06:19:53.319240Z",
     "iopub.status.busy": "2021-11-02T06:19:53.318708Z",
     "iopub.status.idle": "2021-11-02T06:19:54.106484Z",
     "shell.execute_reply": "2021-11-02T06:19:54.105909Z",
     "shell.execute_reply.started": "2021-11-02T06:14:24.999208Z"
    },
    "papermill": {
     "duration": 0.815059,
     "end_time": "2021-11-02T06:19:54.106629",
     "exception": false,
     "start_time": "2021-11-02T06:19:53.291570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tabular-playground-series-nov-2021/sample_submission.csv\n",
      "/kaggle/input/tabular-playground-series-nov-2021/train.csv\n",
      "/kaggle/input/tabular-playground-series-nov-2021/test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "        \n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606e4855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:19:54.142064Z",
     "iopub.status.busy": "2021-11-02T06:19:54.141425Z",
     "iopub.status.idle": "2021-11-02T06:20:23.620824Z",
     "shell.execute_reply": "2021-11-02T06:20:23.620325Z",
     "shell.execute_reply.started": "2021-11-02T06:14:25.856384Z"
    },
    "papermill": {
     "duration": 29.498197,
     "end_time": "2021-11-02T06:20:23.620966",
     "exception": false,
     "start_time": "2021-11-02T06:19:54.122769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/train.csv\")\n",
    "test=pd.read_csv(\"/kaggle/input/tabular-playground-series-nov-2021/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a53233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:20:23.657502Z",
     "iopub.status.busy": "2021-11-02T06:20:23.656552Z",
     "iopub.status.idle": "2021-11-02T06:20:23.660025Z",
     "shell.execute_reply": "2021-11-02T06:20:23.660612Z",
     "shell.execute_reply.started": "2021-11-02T06:14:53.664268Z"
    },
    "papermill": {
     "duration": 0.023985,
     "end_time": "2021-11-02T06:20:23.660792",
     "exception": false,
     "start_time": "2021-11-02T06:20:23.636807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is: (600000, 102)\n",
      "test shape is: (540000, 101)\n"
     ]
    }
   ],
   "source": [
    "print(\"train shape is:\",train.shape)\n",
    "print(\"test shape is:\",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c0cfcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:20:23.701350Z",
     "iopub.status.busy": "2021-11-02T06:20:23.700552Z",
     "iopub.status.idle": "2021-11-02T06:20:23.778912Z",
     "shell.execute_reply": "2021-11-02T06:20:23.778465Z",
     "shell.execute_reply.started": "2021-11-02T06:14:53.672510Z"
    },
    "papermill": {
     "duration": 0.101653,
     "end_time": "2021-11-02T06:20:23.779031",
     "exception": false,
     "start_time": "2021-11-02T06:20:23.677378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>f35</th>\n",
       "      <th>f36</th>\n",
       "      <th>f37</th>\n",
       "      <th>f38</th>\n",
       "      <th>f39</th>\n",
       "      <th>f40</th>\n",
       "      <th>f41</th>\n",
       "      <th>f42</th>\n",
       "      <th>f43</th>\n",
       "      <th>f44</th>\n",
       "      <th>f45</th>\n",
       "      <th>f46</th>\n",
       "      <th>f47</th>\n",
       "      <th>f48</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>f58</th>\n",
       "      <th>f59</th>\n",
       "      <th>f60</th>\n",
       "      <th>f61</th>\n",
       "      <th>f62</th>\n",
       "      <th>f63</th>\n",
       "      <th>f64</th>\n",
       "      <th>f65</th>\n",
       "      <th>f66</th>\n",
       "      <th>f67</th>\n",
       "      <th>f68</th>\n",
       "      <th>f69</th>\n",
       "      <th>f70</th>\n",
       "      <th>f71</th>\n",
       "      <th>f72</th>\n",
       "      <th>f73</th>\n",
       "      <th>f74</th>\n",
       "      <th>f75</th>\n",
       "      <th>f76</th>\n",
       "      <th>f77</th>\n",
       "      <th>f78</th>\n",
       "      <th>f79</th>\n",
       "      <th>f80</th>\n",
       "      <th>f81</th>\n",
       "      <th>f82</th>\n",
       "      <th>f83</th>\n",
       "      <th>f84</th>\n",
       "      <th>f85</th>\n",
       "      <th>f86</th>\n",
       "      <th>f87</th>\n",
       "      <th>f88</th>\n",
       "      <th>f89</th>\n",
       "      <th>f90</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.106643</td>\n",
       "      <td>3.59437</td>\n",
       "      <td>132.8040</td>\n",
       "      <td>3.18428</td>\n",
       "      <td>0.081971</td>\n",
       "      <td>1.18859</td>\n",
       "      <td>3.73238</td>\n",
       "      <td>2.266270</td>\n",
       "      <td>2.09959</td>\n",
       "      <td>0.012330</td>\n",
       "      <td>1.607190</td>\n",
       "      <td>-0.318058</td>\n",
       "      <td>0.560137</td>\n",
       "      <td>2.806880</td>\n",
       "      <td>1.35114</td>\n",
       "      <td>2.535930</td>\n",
       "      <td>0.197527</td>\n",
       "      <td>0.676494</td>\n",
       "      <td>1.98979</td>\n",
       "      <td>-3.842450</td>\n",
       "      <td>0.037380</td>\n",
       "      <td>0.230322</td>\n",
       "      <td>3.33055</td>\n",
       "      <td>0.009397</td>\n",
       "      <td>0.144738</td>\n",
       "      <td>3.05131</td>\n",
       "      <td>1.30362</td>\n",
       "      <td>0.033225</td>\n",
       "      <td>-0.018284</td>\n",
       "      <td>2.748210</td>\n",
       "      <td>-0.009294</td>\n",
       "      <td>-0.036271</td>\n",
       "      <td>-0.049871</td>\n",
       "      <td>0.019484</td>\n",
       "      <td>3.898460</td>\n",
       "      <td>11.2863</td>\n",
       "      <td>1.138020</td>\n",
       "      <td>3.366880</td>\n",
       "      <td>4.94446</td>\n",
       "      <td>-0.105772</td>\n",
       "      <td>2.11345</td>\n",
       "      <td>3.452230</td>\n",
       "      <td>0.789430</td>\n",
       "      <td>1.113210</td>\n",
       "      <td>1.49157</td>\n",
       "      <td>2.440370</td>\n",
       "      <td>0.041824</td>\n",
       "      <td>3.35537</td>\n",
       "      <td>0.053691</td>\n",
       "      <td>1.701270</td>\n",
       "      <td>0.908831</td>\n",
       "      <td>0.094902</td>\n",
       "      <td>0.030219</td>\n",
       "      <td>0.597024</td>\n",
       "      <td>4.443410</td>\n",
       "      <td>1.586490</td>\n",
       "      <td>-0.068686</td>\n",
       "      <td>-0.108268</td>\n",
       "      <td>0.061044</td>\n",
       "      <td>0.046099</td>\n",
       "      <td>0.017113</td>\n",
       "      <td>-0.027551</td>\n",
       "      <td>0.019483</td>\n",
       "      <td>-0.048826</td>\n",
       "      <td>0.050748</td>\n",
       "      <td>3.729300</td>\n",
       "      <td>5.017440</td>\n",
       "      <td>4.186880</td>\n",
       "      <td>0.063342</td>\n",
       "      <td>0.121043</td>\n",
       "      <td>1.37175</td>\n",
       "      <td>4.017450</td>\n",
       "      <td>0.167613</td>\n",
       "      <td>0.039753</td>\n",
       "      <td>2.042360</td>\n",
       "      <td>-0.016614</td>\n",
       "      <td>0.107679</td>\n",
       "      <td>3.507250</td>\n",
       "      <td>0.013660</td>\n",
       "      <td>-0.097023</td>\n",
       "      <td>5.396070</td>\n",
       "      <td>0.244457</td>\n",
       "      <td>3.49184</td>\n",
       "      <td>0.113090</td>\n",
       "      <td>-0.015472</td>\n",
       "      <td>4.208790</td>\n",
       "      <td>4.106560</td>\n",
       "      <td>0.037227</td>\n",
       "      <td>-0.118814</td>\n",
       "      <td>0.067086</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>1.09862</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>-0.011715</td>\n",
       "      <td>0.052759</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>4.211250</td>\n",
       "      <td>1.97877</td>\n",
       "      <td>0.085974</td>\n",
       "      <td>0.240496</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.125021</td>\n",
       "      <td>1.67336</td>\n",
       "      <td>76.5336</td>\n",
       "      <td>3.37825</td>\n",
       "      <td>0.099400</td>\n",
       "      <td>5.09366</td>\n",
       "      <td>1.27562</td>\n",
       "      <td>-0.471318</td>\n",
       "      <td>4.54594</td>\n",
       "      <td>0.037706</td>\n",
       "      <td>0.331749</td>\n",
       "      <td>0.325091</td>\n",
       "      <td>0.062040</td>\n",
       "      <td>2.262150</td>\n",
       "      <td>4.33943</td>\n",
       "      <td>-0.224999</td>\n",
       "      <td>0.233586</td>\n",
       "      <td>3.381280</td>\n",
       "      <td>1.90299</td>\n",
       "      <td>0.067874</td>\n",
       "      <td>-0.051268</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>2.60444</td>\n",
       "      <td>0.103441</td>\n",
       "      <td>0.067638</td>\n",
       "      <td>4.75362</td>\n",
       "      <td>1.85552</td>\n",
       "      <td>-0.181834</td>\n",
       "      <td>0.008359</td>\n",
       "      <td>3.166340</td>\n",
       "      <td>0.011850</td>\n",
       "      <td>0.022292</td>\n",
       "      <td>0.069320</td>\n",
       "      <td>0.117109</td>\n",
       "      <td>0.315276</td>\n",
       "      <td>24.4807</td>\n",
       "      <td>1.672270</td>\n",
       "      <td>-0.409067</td>\n",
       "      <td>4.95475</td>\n",
       "      <td>0.092358</td>\n",
       "      <td>2.60318</td>\n",
       "      <td>1.954690</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>3.289340</td>\n",
       "      <td>2.56453</td>\n",
       "      <td>0.817706</td>\n",
       "      <td>0.025997</td>\n",
       "      <td>4.61749</td>\n",
       "      <td>1.575540</td>\n",
       "      <td>0.066105</td>\n",
       "      <td>0.681634</td>\n",
       "      <td>0.025247</td>\n",
       "      <td>0.183500</td>\n",
       "      <td>0.110038</td>\n",
       "      <td>2.746120</td>\n",
       "      <td>0.835586</td>\n",
       "      <td>0.188196</td>\n",
       "      <td>4.960640</td>\n",
       "      <td>0.136087</td>\n",
       "      <td>-0.008493</td>\n",
       "      <td>-0.015264</td>\n",
       "      <td>-0.010841</td>\n",
       "      <td>0.064584</td>\n",
       "      <td>0.102548</td>\n",
       "      <td>0.093611</td>\n",
       "      <td>0.964089</td>\n",
       "      <td>0.630422</td>\n",
       "      <td>4.307340</td>\n",
       "      <td>0.091289</td>\n",
       "      <td>-0.036360</td>\n",
       "      <td>3.61767</td>\n",
       "      <td>3.103240</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.051302</td>\n",
       "      <td>1.924620</td>\n",
       "      <td>0.123294</td>\n",
       "      <td>-0.022671</td>\n",
       "      <td>1.548120</td>\n",
       "      <td>-0.010397</td>\n",
       "      <td>0.058330</td>\n",
       "      <td>3.661310</td>\n",
       "      <td>-0.118386</td>\n",
       "      <td>2.35739</td>\n",
       "      <td>-0.009112</td>\n",
       "      <td>0.178701</td>\n",
       "      <td>4.097350</td>\n",
       "      <td>3.532890</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>0.121381</td>\n",
       "      <td>0.109968</td>\n",
       "      <td>0.135838</td>\n",
       "      <td>3.46017</td>\n",
       "      <td>0.017054</td>\n",
       "      <td>0.124863</td>\n",
       "      <td>0.154064</td>\n",
       "      <td>0.606848</td>\n",
       "      <td>-0.267928</td>\n",
       "      <td>2.57786</td>\n",
       "      <td>-0.020877</td>\n",
       "      <td>0.024719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.036330</td>\n",
       "      <td>1.49747</td>\n",
       "      <td>233.5460</td>\n",
       "      <td>2.19435</td>\n",
       "      <td>0.026914</td>\n",
       "      <td>3.12694</td>\n",
       "      <td>5.05687</td>\n",
       "      <td>3.849460</td>\n",
       "      <td>1.80187</td>\n",
       "      <td>0.056995</td>\n",
       "      <td>0.328684</td>\n",
       "      <td>2.968810</td>\n",
       "      <td>0.105244</td>\n",
       "      <td>2.069490</td>\n",
       "      <td>5.30986</td>\n",
       "      <td>1.354790</td>\n",
       "      <td>-0.262018</td>\n",
       "      <td>1.379080</td>\n",
       "      <td>1.48091</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>-0.008806</td>\n",
       "      <td>0.109348</td>\n",
       "      <td>1.68365</td>\n",
       "      <td>0.038180</td>\n",
       "      <td>0.123716</td>\n",
       "      <td>1.11248</td>\n",
       "      <td>3.57166</td>\n",
       "      <td>0.120601</td>\n",
       "      <td>0.082069</td>\n",
       "      <td>2.233520</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>0.045182</td>\n",
       "      <td>0.014405</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>-0.502849</td>\n",
       "      <td>33.7382</td>\n",
       "      <td>1.417500</td>\n",
       "      <td>1.071350</td>\n",
       "      <td>3.22296</td>\n",
       "      <td>2.122030</td>\n",
       "      <td>3.08216</td>\n",
       "      <td>0.637555</td>\n",
       "      <td>-0.006822</td>\n",
       "      <td>-0.390943</td>\n",
       "      <td>17.34570</td>\n",
       "      <td>3.700430</td>\n",
       "      <td>-0.033600</td>\n",
       "      <td>1.57824</td>\n",
       "      <td>0.051978</td>\n",
       "      <td>-0.002004</td>\n",
       "      <td>2.690960</td>\n",
       "      <td>0.018367</td>\n",
       "      <td>-0.030468</td>\n",
       "      <td>0.111409</td>\n",
       "      <td>2.187470</td>\n",
       "      <td>-0.325000</td>\n",
       "      <td>-0.019944</td>\n",
       "      <td>3.455030</td>\n",
       "      <td>0.068105</td>\n",
       "      <td>-0.009812</td>\n",
       "      <td>-0.010627</td>\n",
       "      <td>0.027571</td>\n",
       "      <td>-0.007121</td>\n",
       "      <td>-0.048914</td>\n",
       "      <td>-0.002574</td>\n",
       "      <td>1.865090</td>\n",
       "      <td>2.404170</td>\n",
       "      <td>0.411741</td>\n",
       "      <td>0.057749</td>\n",
       "      <td>0.525174</td>\n",
       "      <td>2.16879</td>\n",
       "      <td>0.828297</td>\n",
       "      <td>0.089848</td>\n",
       "      <td>0.093744</td>\n",
       "      <td>4.949010</td>\n",
       "      <td>-0.010978</td>\n",
       "      <td>0.076671</td>\n",
       "      <td>0.266784</td>\n",
       "      <td>0.038691</td>\n",
       "      <td>0.382731</td>\n",
       "      <td>3.847600</td>\n",
       "      <td>-0.121482</td>\n",
       "      <td>3.74084</td>\n",
       "      <td>0.147098</td>\n",
       "      <td>-0.016566</td>\n",
       "      <td>0.614651</td>\n",
       "      <td>2.125840</td>\n",
       "      <td>0.078828</td>\n",
       "      <td>0.979808</td>\n",
       "      <td>0.026758</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>4.88300</td>\n",
       "      <td>0.085222</td>\n",
       "      <td>0.032396</td>\n",
       "      <td>0.116092</td>\n",
       "      <td>-0.001688</td>\n",
       "      <td>-0.520069</td>\n",
       "      <td>2.14112</td>\n",
       "      <td>0.124464</td>\n",
       "      <td>0.148209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.014077</td>\n",
       "      <td>0.24600</td>\n",
       "      <td>779.9670</td>\n",
       "      <td>1.89064</td>\n",
       "      <td>0.006948</td>\n",
       "      <td>1.53112</td>\n",
       "      <td>2.69800</td>\n",
       "      <td>4.517330</td>\n",
       "      <td>4.50332</td>\n",
       "      <td>0.123494</td>\n",
       "      <td>1.002680</td>\n",
       "      <td>4.869600</td>\n",
       "      <td>0.058411</td>\n",
       "      <td>2.497850</td>\n",
       "      <td>1.23843</td>\n",
       "      <td>2.348360</td>\n",
       "      <td>0.175475</td>\n",
       "      <td>1.608890</td>\n",
       "      <td>2.02881</td>\n",
       "      <td>0.042086</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.076506</td>\n",
       "      <td>1.65122</td>\n",
       "      <td>0.111813</td>\n",
       "      <td>0.121641</td>\n",
       "      <td>0.58912</td>\n",
       "      <td>4.23692</td>\n",
       "      <td>-0.032843</td>\n",
       "      <td>0.058168</td>\n",
       "      <td>0.712927</td>\n",
       "      <td>0.097465</td>\n",
       "      <td>0.072744</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.063362</td>\n",
       "      <td>4.063820</td>\n",
       "      <td>25.3824</td>\n",
       "      <td>0.576572</td>\n",
       "      <td>2.026210</td>\n",
       "      <td>2.96843</td>\n",
       "      <td>1.085670</td>\n",
       "      <td>1.71088</td>\n",
       "      <td>1.371820</td>\n",
       "      <td>0.034631</td>\n",
       "      <td>0.722607</td>\n",
       "      <td>71.44850</td>\n",
       "      <td>3.034580</td>\n",
       "      <td>0.092245</td>\n",
       "      <td>3.45310</td>\n",
       "      <td>0.044829</td>\n",
       "      <td>0.027195</td>\n",
       "      <td>4.082850</td>\n",
       "      <td>0.046955</td>\n",
       "      <td>0.063695</td>\n",
       "      <td>0.029225</td>\n",
       "      <td>0.672022</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>0.164284</td>\n",
       "      <td>3.804560</td>\n",
       "      <td>0.062306</td>\n",
       "      <td>-0.021406</td>\n",
       "      <td>0.009471</td>\n",
       "      <td>0.110884</td>\n",
       "      <td>0.026837</td>\n",
       "      <td>2.931160</td>\n",
       "      <td>0.068112</td>\n",
       "      <td>-0.495192</td>\n",
       "      <td>1.345280</td>\n",
       "      <td>2.242750</td>\n",
       "      <td>0.035611</td>\n",
       "      <td>-0.139274</td>\n",
       "      <td>4.74243</td>\n",
       "      <td>3.292740</td>\n",
       "      <td>0.117877</td>\n",
       "      <td>0.065605</td>\n",
       "      <td>0.556711</td>\n",
       "      <td>-0.058029</td>\n",
       "      <td>0.070501</td>\n",
       "      <td>1.101250</td>\n",
       "      <td>0.068559</td>\n",
       "      <td>0.162928</td>\n",
       "      <td>4.070180</td>\n",
       "      <td>-0.008835</td>\n",
       "      <td>3.89678</td>\n",
       "      <td>0.913739</td>\n",
       "      <td>-0.163204</td>\n",
       "      <td>3.074850</td>\n",
       "      <td>4.356780</td>\n",
       "      <td>-0.048894</td>\n",
       "      <td>4.917990</td>\n",
       "      <td>0.069930</td>\n",
       "      <td>-0.015347</td>\n",
       "      <td>3.47439</td>\n",
       "      <td>-0.017103</td>\n",
       "      <td>-0.008100</td>\n",
       "      <td>0.062013</td>\n",
       "      <td>0.041193</td>\n",
       "      <td>0.511657</td>\n",
       "      <td>1.96860</td>\n",
       "      <td>0.040017</td>\n",
       "      <td>0.044873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.003259</td>\n",
       "      <td>3.71542</td>\n",
       "      <td>156.1280</td>\n",
       "      <td>2.14772</td>\n",
       "      <td>0.018284</td>\n",
       "      <td>2.09859</td>\n",
       "      <td>4.15492</td>\n",
       "      <td>-0.038236</td>\n",
       "      <td>3.37145</td>\n",
       "      <td>0.034166</td>\n",
       "      <td>0.711483</td>\n",
       "      <td>0.769988</td>\n",
       "      <td>0.057555</td>\n",
       "      <td>0.957257</td>\n",
       "      <td>3.71145</td>\n",
       "      <td>5.464350</td>\n",
       "      <td>0.287104</td>\n",
       "      <td>2.616950</td>\n",
       "      <td>1.38403</td>\n",
       "      <td>0.074883</td>\n",
       "      <td>-0.010543</td>\n",
       "      <td>0.109121</td>\n",
       "      <td>2.27602</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>0.045235</td>\n",
       "      <td>4.35954</td>\n",
       "      <td>5.07562</td>\n",
       "      <td>-0.009376</td>\n",
       "      <td>0.528966</td>\n",
       "      <td>4.053350</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.106828</td>\n",
       "      <td>0.051307</td>\n",
       "      <td>0.045939</td>\n",
       "      <td>3.402460</td>\n",
       "      <td>15.5615</td>\n",
       "      <td>1.635960</td>\n",
       "      <td>0.047029</td>\n",
       "      <td>4.01771</td>\n",
       "      <td>0.155748</td>\n",
       "      <td>5.28998</td>\n",
       "      <td>4.118920</td>\n",
       "      <td>0.072155</td>\n",
       "      <td>2.752150</td>\n",
       "      <td>3.17116</td>\n",
       "      <td>0.693346</td>\n",
       "      <td>-0.105861</td>\n",
       "      <td>3.32055</td>\n",
       "      <td>0.090698</td>\n",
       "      <td>0.112919</td>\n",
       "      <td>4.619220</td>\n",
       "      <td>0.126866</td>\n",
       "      <td>0.142731</td>\n",
       "      <td>0.055724</td>\n",
       "      <td>4.706800</td>\n",
       "      <td>-0.055123</td>\n",
       "      <td>0.524051</td>\n",
       "      <td>2.971750</td>\n",
       "      <td>0.115334</td>\n",
       "      <td>0.125205</td>\n",
       "      <td>0.067432</td>\n",
       "      <td>0.075586</td>\n",
       "      <td>0.032114</td>\n",
       "      <td>-0.042284</td>\n",
       "      <td>0.047974</td>\n",
       "      <td>-0.294184</td>\n",
       "      <td>5.065600</td>\n",
       "      <td>1.050290</td>\n",
       "      <td>0.034019</td>\n",
       "      <td>0.024611</td>\n",
       "      <td>3.12578</td>\n",
       "      <td>2.262840</td>\n",
       "      <td>0.082462</td>\n",
       "      <td>-0.023296</td>\n",
       "      <td>5.615850</td>\n",
       "      <td>0.086238</td>\n",
       "      <td>0.157568</td>\n",
       "      <td>3.725670</td>\n",
       "      <td>0.061247</td>\n",
       "      <td>0.086603</td>\n",
       "      <td>0.607246</td>\n",
       "      <td>1.411090</td>\n",
       "      <td>2.06062</td>\n",
       "      <td>-0.023154</td>\n",
       "      <td>0.011234</td>\n",
       "      <td>2.155530</td>\n",
       "      <td>0.914518</td>\n",
       "      <td>0.044521</td>\n",
       "      <td>0.375731</td>\n",
       "      <td>0.134351</td>\n",
       "      <td>0.013781</td>\n",
       "      <td>1.91059</td>\n",
       "      <td>-0.042943</td>\n",
       "      <td>0.105616</td>\n",
       "      <td>0.125072</td>\n",
       "      <td>0.037509</td>\n",
       "      <td>1.043790</td>\n",
       "      <td>1.07481</td>\n",
       "      <td>-0.012819</td>\n",
       "      <td>0.072798</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id        f0       f1        f2       f3        f4       f5       f6  \\\n",
       "0   0  0.106643  3.59437  132.8040  3.18428  0.081971  1.18859  3.73238   \n",
       "1   1  0.125021  1.67336   76.5336  3.37825  0.099400  5.09366  1.27562   \n",
       "2   2  0.036330  1.49747  233.5460  2.19435  0.026914  3.12694  5.05687   \n",
       "3   3 -0.014077  0.24600  779.9670  1.89064  0.006948  1.53112  2.69800   \n",
       "4   4 -0.003259  3.71542  156.1280  2.14772  0.018284  2.09859  4.15492   \n",
       "\n",
       "         f7       f8        f9       f10       f11       f12       f13  \\\n",
       "0  2.266270  2.09959  0.012330  1.607190 -0.318058  0.560137  2.806880   \n",
       "1 -0.471318  4.54594  0.037706  0.331749  0.325091  0.062040  2.262150   \n",
       "2  3.849460  1.80187  0.056995  0.328684  2.968810  0.105244  2.069490   \n",
       "3  4.517330  4.50332  0.123494  1.002680  4.869600  0.058411  2.497850   \n",
       "4 -0.038236  3.37145  0.034166  0.711483  0.769988  0.057555  0.957257   \n",
       "\n",
       "       f14       f15       f16       f17      f18       f19       f20  \\\n",
       "0  1.35114  2.535930  0.197527  0.676494  1.98979 -3.842450  0.037380   \n",
       "1  4.33943 -0.224999  0.233586  3.381280  1.90299  0.067874 -0.051268   \n",
       "2  5.30986  1.354790 -0.262018  1.379080  1.48091  0.020542 -0.008806   \n",
       "3  1.23843  2.348360  0.175475  1.608890  2.02881  0.042086  0.005141   \n",
       "4  3.71145  5.464350  0.287104  2.616950  1.38403  0.074883 -0.010543   \n",
       "\n",
       "        f21      f22       f23       f24      f25      f26       f27  \\\n",
       "0  0.230322  3.33055  0.009397  0.144738  3.05131  1.30362  0.033225   \n",
       "1  0.006135  2.60444  0.103441  0.067638  4.75362  1.85552 -0.181834   \n",
       "2  0.109348  1.68365  0.038180  0.123716  1.11248  3.57166  0.120601   \n",
       "3  0.076506  1.65122  0.111813  0.121641  0.58912  4.23692 -0.032843   \n",
       "4  0.109121  2.27602  0.008023  0.045235  4.35954  5.07562 -0.009376   \n",
       "\n",
       "        f28       f29       f30       f31       f32       f33       f34  \\\n",
       "0 -0.018284  2.748210 -0.009294 -0.036271 -0.049871  0.019484  3.898460   \n",
       "1  0.008359  3.166340  0.011850  0.022292  0.069320  0.117109  0.315276   \n",
       "2  0.082069  2.233520  0.002270  0.045182  0.014405  0.011599 -0.502849   \n",
       "3  0.058168  0.712927  0.097465  0.072744  0.000324  0.063362  4.063820   \n",
       "4  0.528966  4.053350  0.020000  0.106828  0.051307  0.045939  3.402460   \n",
       "\n",
       "       f35       f36       f37      f38       f39      f40       f41  \\\n",
       "0  11.2863  1.138020  3.366880  4.94446 -0.105772  2.11345  3.452230   \n",
       "1  24.4807  1.672270 -0.409067  4.95475  0.092358  2.60318  1.954690   \n",
       "2  33.7382  1.417500  1.071350  3.22296  2.122030  3.08216  0.637555   \n",
       "3  25.3824  0.576572  2.026210  2.96843  1.085670  1.71088  1.371820   \n",
       "4  15.5615  1.635960  0.047029  4.01771  0.155748  5.28998  4.118920   \n",
       "\n",
       "        f42       f43       f44       f45       f46      f47       f48  \\\n",
       "0  0.789430  1.113210   1.49157  2.440370  0.041824  3.35537  0.053691   \n",
       "1  0.005896  3.289340   2.56453  0.817706  0.025997  4.61749  1.575540   \n",
       "2 -0.006822 -0.390943  17.34570  3.700430 -0.033600  1.57824  0.051978   \n",
       "3  0.034631  0.722607  71.44850  3.034580  0.092245  3.45310  0.044829   \n",
       "4  0.072155  2.752150   3.17116  0.693346 -0.105861  3.32055  0.090698   \n",
       "\n",
       "        f49       f50       f51       f52       f53       f54       f55  \\\n",
       "0  1.701270  0.908831  0.094902  0.030219  0.597024  4.443410  1.586490   \n",
       "1  0.066105  0.681634  0.025247  0.183500  0.110038  2.746120  0.835586   \n",
       "2 -0.002004  2.690960  0.018367 -0.030468  0.111409  2.187470 -0.325000   \n",
       "3  0.027195  4.082850  0.046955  0.063695  0.029225  0.672022  0.185311   \n",
       "4  0.112919  4.619220  0.126866  0.142731  0.055724  4.706800 -0.055123   \n",
       "\n",
       "        f56       f57       f58       f59       f60       f61       f62  \\\n",
       "0 -0.068686 -0.108268  0.061044  0.046099  0.017113 -0.027551  0.019483   \n",
       "1  0.188196  4.960640  0.136087 -0.008493 -0.015264 -0.010841  0.064584   \n",
       "2 -0.019944  3.455030  0.068105 -0.009812 -0.010627  0.027571 -0.007121   \n",
       "3  0.164284  3.804560  0.062306 -0.021406  0.009471  0.110884  0.026837   \n",
       "4  0.524051  2.971750  0.115334  0.125205  0.067432  0.075586  0.032114   \n",
       "\n",
       "        f63       f64       f65       f66       f67       f68       f69  \\\n",
       "0 -0.048826  0.050748  3.729300  5.017440  4.186880  0.063342  0.121043   \n",
       "1  0.102548  0.093611  0.964089  0.630422  4.307340  0.091289 -0.036360   \n",
       "2 -0.048914 -0.002574  1.865090  2.404170  0.411741  0.057749  0.525174   \n",
       "3  2.931160  0.068112 -0.495192  1.345280  2.242750  0.035611 -0.139274   \n",
       "4 -0.042284  0.047974 -0.294184  5.065600  1.050290  0.034019  0.024611   \n",
       "\n",
       "       f70       f71       f72       f73       f74       f75       f76  \\\n",
       "0  1.37175  4.017450  0.167613  0.039753  2.042360 -0.016614  0.107679   \n",
       "1  3.61767  3.103240  0.000657  0.051302  1.924620  0.123294 -0.022671   \n",
       "2  2.16879  0.828297  0.089848  0.093744  4.949010 -0.010978  0.076671   \n",
       "3  4.74243  3.292740  0.117877  0.065605  0.556711 -0.058029  0.070501   \n",
       "4  3.12578  2.262840  0.082462 -0.023296  5.615850  0.086238  0.157568   \n",
       "\n",
       "        f77       f78       f79       f80       f81      f82       f83  \\\n",
       "0  3.507250  0.013660 -0.097023  5.396070  0.244457  3.49184  0.113090   \n",
       "1  1.548120 -0.010397  0.058330  3.661310 -0.118386  2.35739 -0.009112   \n",
       "2  0.266784  0.038691  0.382731  3.847600 -0.121482  3.74084  0.147098   \n",
       "3  1.101250  0.068559  0.162928  4.070180 -0.008835  3.89678  0.913739   \n",
       "4  3.725670  0.061247  0.086603  0.607246  1.411090  2.06062 -0.023154   \n",
       "\n",
       "        f84       f85       f86       f87       f88       f89       f90  \\\n",
       "0 -0.015472  4.208790  4.106560  0.037227 -0.118814  0.067086  0.010739   \n",
       "1  0.178701  4.097350  3.532890  0.005244  0.121381  0.109968  0.135838   \n",
       "2 -0.016566  0.614651  2.125840  0.078828  0.979808  0.026758  0.117310   \n",
       "3 -0.163204  3.074850  4.356780 -0.048894  4.917990  0.069930 -0.015347   \n",
       "4  0.011234  2.155530  0.914518  0.044521  0.375731  0.134351  0.013781   \n",
       "\n",
       "       f91       f92       f93       f94       f95       f96      f97  \\\n",
       "0  1.09862  0.013331 -0.011715  0.052759  0.065400  4.211250  1.97877   \n",
       "1  3.46017  0.017054  0.124863  0.154064  0.606848 -0.267928  2.57786   \n",
       "2  4.88300  0.085222  0.032396  0.116092 -0.001688 -0.520069  2.14112   \n",
       "3  3.47439 -0.017103 -0.008100  0.062013  0.041193  0.511657  1.96860   \n",
       "4  1.91059 -0.042943  0.105616  0.125072  0.037509  1.043790  1.07481   \n",
       "\n",
       "        f98       f99  target  \n",
       "0  0.085974  0.240496       0  \n",
       "1 -0.020877  0.024719       0  \n",
       "2  0.124464  0.148209       0  \n",
       "3  0.040017  0.044873       0  \n",
       "4 -0.012819  0.072798       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9285ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:20:23.879875Z",
     "iopub.status.busy": "2021-11-02T06:20:23.838381Z",
     "iopub.status.idle": "2021-11-02T06:20:23.901363Z",
     "shell.execute_reply": "2021-11-02T06:20:23.901874Z",
     "shell.execute_reply.started": "2021-11-02T06:14:53.789078Z"
    },
    "papermill": {
     "duration": 0.105705,
     "end_time": "2021-11-02T06:20:23.902036",
     "exception": false,
     "start_time": "2021-11-02T06:20:23.796331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f14</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "      <th>f25</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>f35</th>\n",
       "      <th>f36</th>\n",
       "      <th>f37</th>\n",
       "      <th>f38</th>\n",
       "      <th>f39</th>\n",
       "      <th>f40</th>\n",
       "      <th>f41</th>\n",
       "      <th>f42</th>\n",
       "      <th>f43</th>\n",
       "      <th>f44</th>\n",
       "      <th>f45</th>\n",
       "      <th>f46</th>\n",
       "      <th>f47</th>\n",
       "      <th>f48</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>f58</th>\n",
       "      <th>f59</th>\n",
       "      <th>f60</th>\n",
       "      <th>f61</th>\n",
       "      <th>f62</th>\n",
       "      <th>f63</th>\n",
       "      <th>f64</th>\n",
       "      <th>f65</th>\n",
       "      <th>f66</th>\n",
       "      <th>f67</th>\n",
       "      <th>f68</th>\n",
       "      <th>f69</th>\n",
       "      <th>f70</th>\n",
       "      <th>f71</th>\n",
       "      <th>f72</th>\n",
       "      <th>f73</th>\n",
       "      <th>f74</th>\n",
       "      <th>f75</th>\n",
       "      <th>f76</th>\n",
       "      <th>f77</th>\n",
       "      <th>f78</th>\n",
       "      <th>f79</th>\n",
       "      <th>f80</th>\n",
       "      <th>f81</th>\n",
       "      <th>f82</th>\n",
       "      <th>f83</th>\n",
       "      <th>f84</th>\n",
       "      <th>f85</th>\n",
       "      <th>f86</th>\n",
       "      <th>f87</th>\n",
       "      <th>f88</th>\n",
       "      <th>f89</th>\n",
       "      <th>f90</th>\n",
       "      <th>f91</th>\n",
       "      <th>f92</th>\n",
       "      <th>f93</th>\n",
       "      <th>f94</th>\n",
       "      <th>f95</th>\n",
       "      <th>f96</th>\n",
       "      <th>f97</th>\n",
       "      <th>f98</th>\n",
       "      <th>f99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600000</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>4.838660</td>\n",
       "      <td>585.529</td>\n",
       "      <td>2.282910</td>\n",
       "      <td>0.713180</td>\n",
       "      <td>3.907830</td>\n",
       "      <td>0.480696</td>\n",
       "      <td>1.482270</td>\n",
       "      <td>4.891810</td>\n",
       "      <td>0.056351</td>\n",
       "      <td>4.200990</td>\n",
       "      <td>3.151800</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>1.85116</td>\n",
       "      <td>2.63889</td>\n",
       "      <td>0.746668</td>\n",
       "      <td>-0.004756</td>\n",
       "      <td>1.610300</td>\n",
       "      <td>4.11482</td>\n",
       "      <td>-0.077756</td>\n",
       "      <td>0.129446</td>\n",
       "      <td>0.053324</td>\n",
       "      <td>0.416789</td>\n",
       "      <td>0.445009</td>\n",
       "      <td>0.150464</td>\n",
       "      <td>5.021300</td>\n",
       "      <td>2.22139</td>\n",
       "      <td>-0.072333</td>\n",
       "      <td>-0.215874</td>\n",
       "      <td>1.56236</td>\n",
       "      <td>0.074881</td>\n",
       "      <td>0.010050</td>\n",
       "      <td>0.018582</td>\n",
       "      <td>0.067466</td>\n",
       "      <td>5.57830</td>\n",
       "      <td>3.08556</td>\n",
       "      <td>3.842470</td>\n",
       "      <td>0.011125</td>\n",
       "      <td>2.35997</td>\n",
       "      <td>0.695092</td>\n",
       "      <td>-0.345747</td>\n",
       "      <td>4.38817</td>\n",
       "      <td>0.054919</td>\n",
       "      <td>2.80360</td>\n",
       "      <td>3.87234</td>\n",
       "      <td>3.974520</td>\n",
       "      <td>0.157887</td>\n",
       "      <td>0.704785</td>\n",
       "      <td>-0.001126</td>\n",
       "      <td>-0.004548</td>\n",
       "      <td>1.01082</td>\n",
       "      <td>0.094506</td>\n",
       "      <td>0.016935</td>\n",
       "      <td>0.100871</td>\n",
       "      <td>4.36695</td>\n",
       "      <td>1.832040</td>\n",
       "      <td>0.019682</td>\n",
       "      <td>1.964200</td>\n",
       "      <td>0.120581</td>\n",
       "      <td>0.080247</td>\n",
       "      <td>0.027516</td>\n",
       "      <td>0.048825</td>\n",
       "      <td>0.074626</td>\n",
       "      <td>0.041264</td>\n",
       "      <td>0.069319</td>\n",
       "      <td>4.248810</td>\n",
       "      <td>2.09195</td>\n",
       "      <td>0.695042</td>\n",
       "      <td>0.007464</td>\n",
       "      <td>-0.017882</td>\n",
       "      <td>1.883500</td>\n",
       "      <td>1.268030</td>\n",
       "      <td>0.023747</td>\n",
       "      <td>0.043071</td>\n",
       "      <td>3.174780</td>\n",
       "      <td>0.074356</td>\n",
       "      <td>0.125909</td>\n",
       "      <td>3.926430</td>\n",
       "      <td>0.046914</td>\n",
       "      <td>-0.042290</td>\n",
       "      <td>3.01925</td>\n",
       "      <td>0.089564</td>\n",
       "      <td>3.20070</td>\n",
       "      <td>0.009679</td>\n",
       "      <td>-0.099653</td>\n",
       "      <td>3.573060</td>\n",
       "      <td>4.79727</td>\n",
       "      <td>0.091985</td>\n",
       "      <td>0.773543</td>\n",
       "      <td>0.073380</td>\n",
       "      <td>0.112910</td>\n",
       "      <td>1.073550</td>\n",
       "      <td>0.122149</td>\n",
       "      <td>0.086330</td>\n",
       "      <td>0.036010</td>\n",
       "      <td>0.010619</td>\n",
       "      <td>0.290343</td>\n",
       "      <td>1.898200</td>\n",
       "      <td>0.131533</td>\n",
       "      <td>0.012047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>600001</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.505536</td>\n",
       "      <td>-100.099</td>\n",
       "      <td>3.012670</td>\n",
       "      <td>0.027199</td>\n",
       "      <td>1.194610</td>\n",
       "      <td>5.036620</td>\n",
       "      <td>2.517440</td>\n",
       "      <td>4.553890</td>\n",
       "      <td>0.063876</td>\n",
       "      <td>0.337257</td>\n",
       "      <td>4.439690</td>\n",
       "      <td>0.013188</td>\n",
       "      <td>3.37901</td>\n",
       "      <td>3.38470</td>\n",
       "      <td>1.167400</td>\n",
       "      <td>2.246550</td>\n",
       "      <td>1.750170</td>\n",
       "      <td>2.76624</td>\n",
       "      <td>-0.058501</td>\n",
       "      <td>0.012595</td>\n",
       "      <td>0.036144</td>\n",
       "      <td>0.769057</td>\n",
       "      <td>0.017496</td>\n",
       "      <td>0.050283</td>\n",
       "      <td>0.324697</td>\n",
       "      <td>4.94864</td>\n",
       "      <td>0.124789</td>\n",
       "      <td>0.347128</td>\n",
       "      <td>1.24512</td>\n",
       "      <td>0.035822</td>\n",
       "      <td>-0.013188</td>\n",
       "      <td>0.023194</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>4.98333</td>\n",
       "      <td>23.70690</td>\n",
       "      <td>8.287290</td>\n",
       "      <td>4.796230</td>\n",
       "      <td>1.79928</td>\n",
       "      <td>-0.050040</td>\n",
       "      <td>1.973320</td>\n",
       "      <td>1.91201</td>\n",
       "      <td>0.035974</td>\n",
       "      <td>2.57237</td>\n",
       "      <td>6.32126</td>\n",
       "      <td>0.442628</td>\n",
       "      <td>0.148115</td>\n",
       "      <td>1.311350</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.626128</td>\n",
       "      <td>2.30973</td>\n",
       "      <td>-0.044350</td>\n",
       "      <td>0.194283</td>\n",
       "      <td>0.131445</td>\n",
       "      <td>4.26667</td>\n",
       "      <td>0.715653</td>\n",
       "      <td>0.017117</td>\n",
       "      <td>2.839080</td>\n",
       "      <td>0.104712</td>\n",
       "      <td>-0.007952</td>\n",
       "      <td>0.067650</td>\n",
       "      <td>0.066438</td>\n",
       "      <td>0.034258</td>\n",
       "      <td>0.606144</td>\n",
       "      <td>0.019988</td>\n",
       "      <td>-0.456704</td>\n",
       "      <td>3.37877</td>\n",
       "      <td>1.900050</td>\n",
       "      <td>0.052316</td>\n",
       "      <td>-0.050397</td>\n",
       "      <td>2.612950</td>\n",
       "      <td>3.112540</td>\n",
       "      <td>0.022305</td>\n",
       "      <td>0.108325</td>\n",
       "      <td>4.617640</td>\n",
       "      <td>0.091440</td>\n",
       "      <td>0.039368</td>\n",
       "      <td>3.035190</td>\n",
       "      <td>1.023050</td>\n",
       "      <td>-0.020578</td>\n",
       "      <td>2.77156</td>\n",
       "      <td>3.309480</td>\n",
       "      <td>5.05840</td>\n",
       "      <td>-0.003993</td>\n",
       "      <td>0.067636</td>\n",
       "      <td>0.995391</td>\n",
       "      <td>2.47165</td>\n",
       "      <td>-0.015214</td>\n",
       "      <td>0.263423</td>\n",
       "      <td>-0.021910</td>\n",
       "      <td>-0.020214</td>\n",
       "      <td>2.622340</td>\n",
       "      <td>0.123307</td>\n",
       "      <td>0.033063</td>\n",
       "      <td>0.123059</td>\n",
       "      <td>0.005771</td>\n",
       "      <td>-0.392923</td>\n",
       "      <td>3.689640</td>\n",
       "      <td>0.047418</td>\n",
       "      <td>0.120015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>600002</td>\n",
       "      <td>1.461000</td>\n",
       "      <td>2.437260</td>\n",
       "      <td>-112.964</td>\n",
       "      <td>3.541230</td>\n",
       "      <td>0.752338</td>\n",
       "      <td>4.338310</td>\n",
       "      <td>1.648080</td>\n",
       "      <td>4.699910</td>\n",
       "      <td>1.950250</td>\n",
       "      <td>0.005303</td>\n",
       "      <td>2.071680</td>\n",
       "      <td>0.546499</td>\n",
       "      <td>0.141781</td>\n",
       "      <td>1.67317</td>\n",
       "      <td>4.30649</td>\n",
       "      <td>1.702330</td>\n",
       "      <td>-0.062869</td>\n",
       "      <td>1.619230</td>\n",
       "      <td>4.19053</td>\n",
       "      <td>0.055140</td>\n",
       "      <td>-0.016590</td>\n",
       "      <td>0.017805</td>\n",
       "      <td>3.064810</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>0.098316</td>\n",
       "      <td>3.507540</td>\n",
       "      <td>1.06910</td>\n",
       "      <td>0.012750</td>\n",
       "      <td>0.009981</td>\n",
       "      <td>3.46781</td>\n",
       "      <td>0.035920</td>\n",
       "      <td>-0.009804</td>\n",
       "      <td>0.065728</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>5.28102</td>\n",
       "      <td>11.52880</td>\n",
       "      <td>0.171694</td>\n",
       "      <td>4.394570</td>\n",
       "      <td>2.52084</td>\n",
       "      <td>0.079365</td>\n",
       "      <td>5.451320</td>\n",
       "      <td>1.78582</td>\n",
       "      <td>0.029620</td>\n",
       "      <td>3.06383</td>\n",
       "      <td>5.27599</td>\n",
       "      <td>0.315972</td>\n",
       "      <td>0.148310</td>\n",
       "      <td>0.463193</td>\n",
       "      <td>-0.039962</td>\n",
       "      <td>0.150592</td>\n",
       "      <td>3.75460</td>\n",
       "      <td>0.024164</td>\n",
       "      <td>0.107455</td>\n",
       "      <td>0.048741</td>\n",
       "      <td>3.58240</td>\n",
       "      <td>0.966391</td>\n",
       "      <td>-0.058947</td>\n",
       "      <td>1.869830</td>\n",
       "      <td>0.029872</td>\n",
       "      <td>-0.044294</td>\n",
       "      <td>0.065172</td>\n",
       "      <td>-0.003328</td>\n",
       "      <td>-0.001950</td>\n",
       "      <td>0.095007</td>\n",
       "      <td>0.080267</td>\n",
       "      <td>3.337300</td>\n",
       "      <td>3.60489</td>\n",
       "      <td>4.694580</td>\n",
       "      <td>0.075650</td>\n",
       "      <td>-0.088276</td>\n",
       "      <td>1.772440</td>\n",
       "      <td>-0.142226</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>0.241844</td>\n",
       "      <td>0.067591</td>\n",
       "      <td>-0.026572</td>\n",
       "      <td>0.894909</td>\n",
       "      <td>0.111606</td>\n",
       "      <td>0.524423</td>\n",
       "      <td>1.45007</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>1.27135</td>\n",
       "      <td>0.076042</td>\n",
       "      <td>0.446993</td>\n",
       "      <td>4.406990</td>\n",
       "      <td>2.44697</td>\n",
       "      <td>-0.041154</td>\n",
       "      <td>0.212414</td>\n",
       "      <td>0.141005</td>\n",
       "      <td>-0.011036</td>\n",
       "      <td>2.030180</td>\n",
       "      <td>-0.000426</td>\n",
       "      <td>0.084091</td>\n",
       "      <td>0.123605</td>\n",
       "      <td>0.499554</td>\n",
       "      <td>4.054650</td>\n",
       "      <td>3.330670</td>\n",
       "      <td>0.108843</td>\n",
       "      <td>0.064687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>600003</td>\n",
       "      <td>0.140556</td>\n",
       "      <td>3.085610</td>\n",
       "      <td>179.451</td>\n",
       "      <td>0.573945</td>\n",
       "      <td>0.057342</td>\n",
       "      <td>2.216790</td>\n",
       "      <td>1.623480</td>\n",
       "      <td>0.526174</td>\n",
       "      <td>1.542540</td>\n",
       "      <td>-0.026160</td>\n",
       "      <td>1.609440</td>\n",
       "      <td>1.723560</td>\n",
       "      <td>-0.019564</td>\n",
       "      <td>1.55213</td>\n",
       "      <td>4.83264</td>\n",
       "      <td>1.501640</td>\n",
       "      <td>0.192669</td>\n",
       "      <td>4.614890</td>\n",
       "      <td>1.47069</td>\n",
       "      <td>-0.010031</td>\n",
       "      <td>0.072805</td>\n",
       "      <td>0.048035</td>\n",
       "      <td>3.230210</td>\n",
       "      <td>-0.031548</td>\n",
       "      <td>0.028697</td>\n",
       "      <td>3.752520</td>\n",
       "      <td>4.94847</td>\n",
       "      <td>-0.174542</td>\n",
       "      <td>-0.033491</td>\n",
       "      <td>2.47823</td>\n",
       "      <td>0.068129</td>\n",
       "      <td>0.090797</td>\n",
       "      <td>0.029877</td>\n",
       "      <td>0.146718</td>\n",
       "      <td>3.16983</td>\n",
       "      <td>-12.98460</td>\n",
       "      <td>3.135210</td>\n",
       "      <td>1.765010</td>\n",
       "      <td>3.25399</td>\n",
       "      <td>0.713238</td>\n",
       "      <td>3.729920</td>\n",
       "      <td>2.65153</td>\n",
       "      <td>0.080945</td>\n",
       "      <td>1.05649</td>\n",
       "      <td>1.45260</td>\n",
       "      <td>1.814190</td>\n",
       "      <td>0.077875</td>\n",
       "      <td>2.721310</td>\n",
       "      <td>0.004530</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>2.83445</td>\n",
       "      <td>0.024642</td>\n",
       "      <td>-0.051423</td>\n",
       "      <td>0.060574</td>\n",
       "      <td>3.42020</td>\n",
       "      <td>1.283600</td>\n",
       "      <td>-0.083213</td>\n",
       "      <td>0.048820</td>\n",
       "      <td>-0.002502</td>\n",
       "      <td>0.051387</td>\n",
       "      <td>0.075775</td>\n",
       "      <td>-0.008900</td>\n",
       "      <td>0.141776</td>\n",
       "      <td>0.173123</td>\n",
       "      <td>0.012894</td>\n",
       "      <td>3.390590</td>\n",
       "      <td>2.69899</td>\n",
       "      <td>0.059916</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>2.641440</td>\n",
       "      <td>0.839721</td>\n",
       "      <td>-0.026268</td>\n",
       "      <td>0.058745</td>\n",
       "      <td>2.521910</td>\n",
       "      <td>0.118737</td>\n",
       "      <td>0.134899</td>\n",
       "      <td>2.839480</td>\n",
       "      <td>0.108751</td>\n",
       "      <td>-0.015848</td>\n",
       "      <td>1.14724</td>\n",
       "      <td>0.390064</td>\n",
       "      <td>3.30923</td>\n",
       "      <td>-0.046117</td>\n",
       "      <td>-0.206052</td>\n",
       "      <td>2.373770</td>\n",
       "      <td>3.15941</td>\n",
       "      <td>0.131234</td>\n",
       "      <td>1.031180</td>\n",
       "      <td>0.026155</td>\n",
       "      <td>0.050117</td>\n",
       "      <td>0.221613</td>\n",
       "      <td>0.045298</td>\n",
       "      <td>0.129966</td>\n",
       "      <td>0.004015</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>2.696580</td>\n",
       "      <td>-0.533491</td>\n",
       "      <td>0.052524</td>\n",
       "      <td>0.011058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>600004</td>\n",
       "      <td>0.128876</td>\n",
       "      <td>5.199760</td>\n",
       "      <td>107.466</td>\n",
       "      <td>-0.497149</td>\n",
       "      <td>0.080220</td>\n",
       "      <td>0.458121</td>\n",
       "      <td>0.629839</td>\n",
       "      <td>5.240460</td>\n",
       "      <td>-0.232279</td>\n",
       "      <td>0.030006</td>\n",
       "      <td>0.481359</td>\n",
       "      <td>2.176020</td>\n",
       "      <td>0.193162</td>\n",
       "      <td>1.39209</td>\n",
       "      <td>2.51890</td>\n",
       "      <td>2.993170</td>\n",
       "      <td>4.170910</td>\n",
       "      <td>0.318375</td>\n",
       "      <td>4.84563</td>\n",
       "      <td>0.085064</td>\n",
       "      <td>0.026443</td>\n",
       "      <td>-0.004559</td>\n",
       "      <td>0.120327</td>\n",
       "      <td>-0.008630</td>\n",
       "      <td>0.004495</td>\n",
       "      <td>4.921700</td>\n",
       "      <td>1.68564</td>\n",
       "      <td>0.095628</td>\n",
       "      <td>0.189131</td>\n",
       "      <td>2.56955</td>\n",
       "      <td>0.046643</td>\n",
       "      <td>0.111462</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.060737</td>\n",
       "      <td>4.25300</td>\n",
       "      <td>38.70340</td>\n",
       "      <td>0.170825</td>\n",
       "      <td>-0.598784</td>\n",
       "      <td>3.92796</td>\n",
       "      <td>0.262956</td>\n",
       "      <td>5.190270</td>\n",
       "      <td>1.99354</td>\n",
       "      <td>-0.007611</td>\n",
       "      <td>1.09235</td>\n",
       "      <td>1.64661</td>\n",
       "      <td>5.144700</td>\n",
       "      <td>0.173642</td>\n",
       "      <td>1.156500</td>\n",
       "      <td>-0.016784</td>\n",
       "      <td>-0.045914</td>\n",
       "      <td>1.29085</td>\n",
       "      <td>0.058223</td>\n",
       "      <td>-0.147922</td>\n",
       "      <td>0.104325</td>\n",
       "      <td>3.69047</td>\n",
       "      <td>1.456270</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.635948</td>\n",
       "      <td>0.039901</td>\n",
       "      <td>-0.059999</td>\n",
       "      <td>0.123505</td>\n",
       "      <td>0.011181</td>\n",
       "      <td>0.152410</td>\n",
       "      <td>0.118767</td>\n",
       "      <td>-0.054951</td>\n",
       "      <td>4.841230</td>\n",
       "      <td>3.39522</td>\n",
       "      <td>3.737320</td>\n",
       "      <td>0.053328</td>\n",
       "      <td>0.143619</td>\n",
       "      <td>-0.156581</td>\n",
       "      <td>-0.257201</td>\n",
       "      <td>0.987785</td>\n",
       "      <td>0.038284</td>\n",
       "      <td>2.765120</td>\n",
       "      <td>0.040994</td>\n",
       "      <td>0.173981</td>\n",
       "      <td>1.112190</td>\n",
       "      <td>-0.030675</td>\n",
       "      <td>0.137178</td>\n",
       "      <td>1.55014</td>\n",
       "      <td>0.401083</td>\n",
       "      <td>1.74053</td>\n",
       "      <td>0.096331</td>\n",
       "      <td>0.445268</td>\n",
       "      <td>4.392460</td>\n",
       "      <td>2.52818</td>\n",
       "      <td>-0.037385</td>\n",
       "      <td>0.117148</td>\n",
       "      <td>-0.010128</td>\n",
       "      <td>0.058860</td>\n",
       "      <td>2.660430</td>\n",
       "      <td>0.135425</td>\n",
       "      <td>0.036481</td>\n",
       "      <td>0.093912</td>\n",
       "      <td>0.056315</td>\n",
       "      <td>1.110710</td>\n",
       "      <td>3.584470</td>\n",
       "      <td>0.145319</td>\n",
       "      <td>-0.050393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id        f0        f1       f2        f3        f4        f5  \\\n",
       "0  600000  0.003229  4.838660  585.529  2.282910  0.713180  3.907830   \n",
       "1  600001  0.008602  0.505536 -100.099  3.012670  0.027199  1.194610   \n",
       "2  600002  1.461000  2.437260 -112.964  3.541230  0.752338  4.338310   \n",
       "3  600003  0.140556  3.085610  179.451  0.573945  0.057342  2.216790   \n",
       "4  600004  0.128876  5.199760  107.466 -0.497149  0.080220  0.458121   \n",
       "\n",
       "         f6        f7        f8        f9       f10       f11       f12  \\\n",
       "0  0.480696  1.482270  4.891810  0.056351  4.200990  3.151800  0.000349   \n",
       "1  5.036620  2.517440  4.553890  0.063876  0.337257  4.439690  0.013188   \n",
       "2  1.648080  4.699910  1.950250  0.005303  2.071680  0.546499  0.141781   \n",
       "3  1.623480  0.526174  1.542540 -0.026160  1.609440  1.723560 -0.019564   \n",
       "4  0.629839  5.240460 -0.232279  0.030006  0.481359  2.176020  0.193162   \n",
       "\n",
       "       f13      f14       f15       f16       f17      f18       f19  \\\n",
       "0  1.85116  2.63889  0.746668 -0.004756  1.610300  4.11482 -0.077756   \n",
       "1  3.37901  3.38470  1.167400  2.246550  1.750170  2.76624 -0.058501   \n",
       "2  1.67317  4.30649  1.702330 -0.062869  1.619230  4.19053  0.055140   \n",
       "3  1.55213  4.83264  1.501640  0.192669  4.614890  1.47069 -0.010031   \n",
       "4  1.39209  2.51890  2.993170  4.170910  0.318375  4.84563  0.085064   \n",
       "\n",
       "        f20       f21       f22       f23       f24       f25      f26  \\\n",
       "0  0.129446  0.053324  0.416789  0.445009  0.150464  5.021300  2.22139   \n",
       "1  0.012595  0.036144  0.769057  0.017496  0.050283  0.324697  4.94864   \n",
       "2 -0.016590  0.017805  3.064810  0.070370  0.098316  3.507540  1.06910   \n",
       "3  0.072805  0.048035  3.230210 -0.031548  0.028697  3.752520  4.94847   \n",
       "4  0.026443 -0.004559  0.120327 -0.008630  0.004495  4.921700  1.68564   \n",
       "\n",
       "        f27       f28      f29       f30       f31       f32       f33  \\\n",
       "0 -0.072333 -0.215874  1.56236  0.074881  0.010050  0.018582  0.067466   \n",
       "1  0.124789  0.347128  1.24512  0.035822 -0.013188  0.023194  0.006444   \n",
       "2  0.012750  0.009981  3.46781  0.035920 -0.009804  0.065728 -0.004725   \n",
       "3 -0.174542 -0.033491  2.47823  0.068129  0.090797  0.029877  0.146718   \n",
       "4  0.095628  0.189131  2.56955  0.046643  0.111462  0.002912  0.060737   \n",
       "\n",
       "       f34       f35       f36       f37      f38       f39       f40  \\\n",
       "0  5.57830   3.08556  3.842470  0.011125  2.35997  0.695092 -0.345747   \n",
       "1  4.98333  23.70690  8.287290  4.796230  1.79928 -0.050040  1.973320   \n",
       "2  5.28102  11.52880  0.171694  4.394570  2.52084  0.079365  5.451320   \n",
       "3  3.16983 -12.98460  3.135210  1.765010  3.25399  0.713238  3.729920   \n",
       "4  4.25300  38.70340  0.170825 -0.598784  3.92796  0.262956  5.190270   \n",
       "\n",
       "       f41       f42      f43      f44       f45       f46       f47  \\\n",
       "0  4.38817  0.054919  2.80360  3.87234  3.974520  0.157887  0.704785   \n",
       "1  1.91201  0.035974  2.57237  6.32126  0.442628  0.148115  1.311350   \n",
       "2  1.78582  0.029620  3.06383  5.27599  0.315972  0.148310  0.463193   \n",
       "3  2.65153  0.080945  1.05649  1.45260  1.814190  0.077875  2.721310   \n",
       "4  1.99354 -0.007611  1.09235  1.64661  5.144700  0.173642  1.156500   \n",
       "\n",
       "        f48       f49      f50       f51       f52       f53      f54  \\\n",
       "0 -0.001126 -0.004548  1.01082  0.094506  0.016935  0.100871  4.36695   \n",
       "1  0.106667  0.626128  2.30973 -0.044350  0.194283  0.131445  4.26667   \n",
       "2 -0.039962  0.150592  3.75460  0.024164  0.107455  0.048741  3.58240   \n",
       "3  0.004530  0.108844  2.83445  0.024642 -0.051423  0.060574  3.42020   \n",
       "4 -0.016784 -0.045914  1.29085  0.058223 -0.147922  0.104325  3.69047   \n",
       "\n",
       "        f55       f56       f57       f58       f59       f60       f61  \\\n",
       "0  1.832040  0.019682  1.964200  0.120581  0.080247  0.027516  0.048825   \n",
       "1  0.715653  0.017117  2.839080  0.104712 -0.007952  0.067650  0.066438   \n",
       "2  0.966391 -0.058947  1.869830  0.029872 -0.044294  0.065172 -0.003328   \n",
       "3  1.283600 -0.083213  0.048820 -0.002502  0.051387  0.075775 -0.008900   \n",
       "4  1.456270  0.008108  0.635948  0.039901 -0.059999  0.123505  0.011181   \n",
       "\n",
       "        f62       f63       f64       f65      f66       f67       f68  \\\n",
       "0  0.074626  0.041264  0.069319  4.248810  2.09195  0.695042  0.007464   \n",
       "1  0.034258  0.606144  0.019988 -0.456704  3.37877  1.900050  0.052316   \n",
       "2 -0.001950  0.095007  0.080267  3.337300  3.60489  4.694580  0.075650   \n",
       "3  0.141776  0.173123  0.012894  3.390590  2.69899  0.059916  0.055400   \n",
       "4  0.152410  0.118767 -0.054951  4.841230  3.39522  3.737320  0.053328   \n",
       "\n",
       "        f69       f70       f71       f72       f73       f74       f75  \\\n",
       "0 -0.017882  1.883500  1.268030  0.023747  0.043071  3.174780  0.074356   \n",
       "1 -0.050397  2.612950  3.112540  0.022305  0.108325  4.617640  0.091440   \n",
       "2 -0.088276  1.772440 -0.142226  0.000918  0.021483  0.241844  0.067591   \n",
       "3 -0.000140  2.641440  0.839721 -0.026268  0.058745  2.521910  0.118737   \n",
       "4  0.143619 -0.156581 -0.257201  0.987785  0.038284  2.765120  0.040994   \n",
       "\n",
       "        f76       f77       f78       f79      f80       f81      f82  \\\n",
       "0  0.125909  3.926430  0.046914 -0.042290  3.01925  0.089564  3.20070   \n",
       "1  0.039368  3.035190  1.023050 -0.020578  2.77156  3.309480  5.05840   \n",
       "2 -0.026572  0.894909  0.111606  0.524423  1.45007  0.019355  1.27135   \n",
       "3  0.134899  2.839480  0.108751 -0.015848  1.14724  0.390064  3.30923   \n",
       "4  0.173981  1.112190 -0.030675  0.137178  1.55014  0.401083  1.74053   \n",
       "\n",
       "        f83       f84       f85      f86       f87       f88       f89  \\\n",
       "0  0.009679 -0.099653  3.573060  4.79727  0.091985  0.773543  0.073380   \n",
       "1 -0.003993  0.067636  0.995391  2.47165 -0.015214  0.263423 -0.021910   \n",
       "2  0.076042  0.446993  4.406990  2.44697 -0.041154  0.212414  0.141005   \n",
       "3 -0.046117 -0.206052  2.373770  3.15941  0.131234  1.031180  0.026155   \n",
       "4  0.096331  0.445268  4.392460  2.52818 -0.037385  0.117148 -0.010128   \n",
       "\n",
       "        f90       f91       f92       f93       f94       f95       f96  \\\n",
       "0  0.112910  1.073550  0.122149  0.086330  0.036010  0.010619  0.290343   \n",
       "1 -0.020214  2.622340  0.123307  0.033063  0.123059  0.005771 -0.392923   \n",
       "2 -0.011036  2.030180 -0.000426  0.084091  0.123605  0.499554  4.054650   \n",
       "3  0.050117  0.221613  0.045298  0.129966  0.004015  0.018279  2.696580   \n",
       "4  0.058860  2.660430  0.135425  0.036481  0.093912  0.056315  1.110710   \n",
       "\n",
       "        f97       f98       f99  \n",
       "0  1.898200  0.131533  0.012047  \n",
       "1  3.689640  0.047418  0.120015  \n",
       "2  3.330670  0.108843  0.064687  \n",
       "3 -0.533491  0.052524  0.011058  \n",
       "4  3.584470  0.145319 -0.050393  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27915385",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:20:23.946716Z",
     "iopub.status.busy": "2021-11-02T06:20:23.945913Z",
     "iopub.status.idle": "2021-11-02T06:20:24.182259Z",
     "shell.execute_reply": "2021-11-02T06:20:24.182874Z",
     "shell.execute_reply.started": "2021-11-02T06:14:53.921970Z"
    },
    "papermill": {
     "duration": 0.261569,
     "end_time": "2021-11-02T06:20:24.183096",
     "exception": false,
     "start_time": "2021-11-02T06:20:23.921527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    303606\n",
      "Name: target, dtype: int64\n",
      "0    296394\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train[train['target']==1]['target'].value_counts())\n",
    "print(train[train['target']==0]['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d826888",
   "metadata": {
    "papermill": {
     "duration": 0.017634,
     "end_time": "2021-11-02T06:20:24.219714",
     "exception": false,
     "start_time": "2021-11-02T06:20:24.202080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# OBSERVATIONS\n",
    "1. **No null value is present in the dataset**\n",
    "2. **data has 102 columns/features including target**\n",
    "3. **all 100 features are float variables**\n",
    "4. **The given problem is of binary classification.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bae8b",
   "metadata": {
    "papermill": {
     "duration": 0.018074,
     "end_time": "2021-11-02T06:20:24.255760",
     "exception": false,
     "start_time": "2021-11-02T06:20:24.237686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **IMPORT TENSORFLOW KERAS LIBRARIES**\n",
    "\n",
    "Here i have define 2 things:\n",
    "\n",
    "* early stopping:if the model auc doesn't improve for 20 iterations training of that particular model will be stop\n",
    "\n",
    "* Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 0.2 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da47f77b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:20:24.298613Z",
     "iopub.status.busy": "2021-11-02T06:20:24.298049Z",
     "iopub.status.idle": "2021-11-02T06:20:28.664480Z",
     "shell.execute_reply": "2021-11-02T06:20:28.663996Z",
     "shell.execute_reply.started": "2021-11-02T06:14:54.221078Z"
    },
    "papermill": {
     "duration": 4.390339,
     "end_time": "2021-11-02T06:20:28.664613",
     "exception": false,
     "start_time": "2021-11-02T06:20:24.274274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=5, verbose=0,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e9cdc",
   "metadata": {
    "papermill": {
     "duration": 0.019066,
     "end_time": "2021-11-02T06:20:28.702686",
     "exception": false,
     "start_time": "2021-11-02T06:20:28.683620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Quantile Transformation**\n",
    "Transform features using quantiles information.\n",
    "\n",
    "This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.\n",
    "\n",
    "The transformation is applied on each feature independently. First an estimate of the cumulative distribution function of a feature is used to map the original values to a uniform distribution. The obtained values are then mapped to the desired output distribution using the associated quantile function. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423af92",
   "metadata": {
    "papermill": {
     "duration": 0.017533,
     "end_time": "2021-11-02T06:20:28.738000",
     "exception": false,
     "start_time": "2021-11-02T06:20:28.720467",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Uncomment the last 6 lines if you want to try exploring Quantile transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488f9f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:20:28.781989Z",
     "iopub.status.busy": "2021-11-02T06:20:28.780521Z",
     "iopub.status.idle": "2021-11-02T06:20:29.757089Z",
     "shell.execute_reply": "2021-11-02T06:20:29.757530Z",
     "shell.execute_reply.started": "2021-11-02T06:14:58.747420Z"
    },
    "papermill": {
     "duration": 1.00194,
     "end_time": "2021-11-02T06:20:29.757724",
     "exception": false,
     "start_time": "2021-11-02T06:20:28.755784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "colNames=[col for col in test.columns if col not in 'id']\n",
    "# colNames.remove('id')\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "qt_train = []\n",
    "train_nn=train[colNames].copy()\n",
    "test_nn=test[colNames].copy()\n",
    "# for col in colNames:\n",
    "#     #print(col)\n",
    "#     qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
    "#     train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "#     test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "#     qt_train.append(qt)\n",
    "train_nn['target']=train['target']"
   ]
  },
  {
   "attachments": {
    "7e2ce760-5f4a-432a-ae88-1fe400f68eab.png": {
     "image/png": "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"
    },
    "aa68cdce-9fee-422e-a1e0-ead3785b705e.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5be52c45",
   "metadata": {
    "papermill": {
     "duration": 0.018166,
     "end_time": "2021-11-02T06:20:29.794056",
     "exception": false,
     "start_time": "2021-11-02T06:20:29.775890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **KEY POINTS:**\n",
    "* **Swish Activation**:The choice of activation functions in Deep Neural Networks has a significant impact on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU), which is f(x)=max(0,x). Although various alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. So Google Brain Team has proposed a new activation function, named Swish, which is simply f(x) = x  sigmoid(x). Their experiments show that Swish tends to work better than ReLU on deeper models across a number of challenging datasets. \n",
    "\n",
    "![image.png](attachment:aa68cdce-9fee-422e-a1e0-ead3785b705e.png)\n",
    "\n",
    "* **Input Layer** : every neural network has an input layer, we have passed shape=(100,) to it since there are 100 features in our data.\n",
    "\n",
    "* **Hidden Layers** :In neural networks, a hidden layer is located between the input and output of the algorithm, in which the function applies weights to the inputs and directs them through an activation function as the output. In short, the hidden layers perform nonlinear transformations of the inputs entered into the network. here we have three hidden layers of units (100,64,32) units.\n",
    "\n",
    "* **Output Layers** :There must always be one output layer in a neural network. The output layer takes in the inputs which are passed in from the layers before it, performs the calculations via its neurons and then the output is computed. here we have used sigmoid activation.Image below is the diagram of sigmid activation.\n",
    "\n",
    "![image.png](attachment:7e2ce760-5f4a-432a-ae88-1fe400f68eab.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddabeb3e",
   "metadata": {
    "papermill": {
     "duration": 0.036054,
     "end_time": "2021-11-02T06:20:29.862341",
     "exception": false,
     "start_time": "2021-11-02T06:20:29.826287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c79a7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:20:29.952373Z",
     "iopub.status.busy": "2021-11-02T06:20:29.949091Z",
     "iopub.status.idle": "2021-11-02T06:20:29.993322Z",
     "shell.execute_reply": "2021-11-02T06:20:29.990095Z",
     "shell.execute_reply.started": "2021-11-02T06:14:59.851164Z"
    },
    "papermill": {
     "duration": 0.098263,
     "end_time": "2021-11-02T06:20:29.993509",
     "exception": false,
     "start_time": "2021-11-02T06:20:29.895246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://bignerdranch.com/blog/implementing-swish-activation-function-in-keras/\n",
    "from keras.backend import sigmoid\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (100,64,32)#initialized number of neurons in each hidden layer\n",
    "\n",
    "def base_model():\n",
    "\n",
    "    num_input = keras.Input(shape=(100,), name='num_data')#input layer\n",
    "\n",
    "\n",
    "    out = keras.layers.Concatenate()([num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "        \n",
    "\n",
    "    #out = keras.layers.Concatenate()([out, num_input])\n",
    "\n",
    "    # A single output: our predicted target value probability\n",
    "    out = keras.layers.Dense(1, activation='sigmoid', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "attachments": {
    "a780c9e1-1c64-4292-ba2e-7493ee12d7bf.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a42c9327",
   "metadata": {
    "papermill": {
     "duration": 0.037767,
     "end_time": "2021-11-02T06:20:30.091524",
     "exception": false,
     "start_time": "2021-11-02T06:20:30.053757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Key points:**\n",
    "\n",
    "*  **Validation strategy**: here i have used Kfold strategy, the data is divided into 5 folds , 4 of them are used for training and 1 of them is used for validation this will happen 5 times.\n",
    "* **Loss=binary cross entropy**: for understanding this you can refer this link :https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "![image.png](attachment:a780c9e1-1c64-4292-ba2e-7493ee12d7bf.png)\n",
    "* **Metrics=AUC**:This is the metrics we have to use in this competition.\n",
    "\n",
    "* **Batch size**:Batch size is a term used in machine learning and refers to the number of training examples utilized in one iteration. so our training examples are going into our network in batches here specifically they are going in the batches of 2048, you can try tweaking it and check what happens if we change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d28e717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:20:30.493052Z",
     "iopub.status.busy": "2021-11-02T06:20:30.491929Z",
     "iopub.status.idle": "2021-11-02T06:32:34.630354Z",
     "shell.execute_reply": "2021-11-02T06:32:34.629855Z"
    },
    "papermill": {
     "duration": 724.504033,
     "end_time": "2021-11-02T06:32:34.630497",
     "exception": false,
     "start_time": "2021-11-02T06:20:30.126464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "CV 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-02 06:20:32.797724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 06:20:32.885402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 06:20:32.886114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 06:20:32.887198: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-02 06:20:32.888287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 06:20:32.888955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 06:20:32.889564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 06:20:34.479497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 06:20:34.480471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 06:20:34.481219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-02 06:20:34.481836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n",
      "2021-11-02 06:20:35.980102: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "235/235 [==============================] - 3s 6ms/step - loss: 0.6298 - auc: 0.7153 - val_loss: 0.6151 - val_auc: 0.7355\n",
      "Epoch 2/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6155 - auc: 0.7336 - val_loss: 0.6143 - val_auc: 0.7397\n",
      "Epoch 3/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6141 - auc: 0.7359 - val_loss: 0.6123 - val_auc: 0.7420\n",
      "Epoch 4/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.6127 - auc: 0.7378 - val_loss: 0.6087 - val_auc: 0.7435\n",
      "Epoch 5/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6074 - auc: 0.7388 - val_loss: 0.5972 - val_auc: 0.7430\n",
      "Epoch 6/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5962 - auc: 0.7401 - val_loss: 0.5917 - val_auc: 0.7439\n",
      "Epoch 7/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5929 - auc: 0.7417 - val_loss: 0.5884 - val_auc: 0.7455\n",
      "Epoch 8/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5897 - auc: 0.7438 - val_loss: 0.5864 - val_auc: 0.7463\n",
      "Epoch 9/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5880 - auc: 0.7448 - val_loss: 0.5909 - val_auc: 0.7463\n",
      "Epoch 10/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5868 - auc: 0.7454 - val_loss: 0.5852 - val_auc: 0.7482\n",
      "Epoch 11/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5863 - auc: 0.7457 - val_loss: 0.5820 - val_auc: 0.7487\n",
      "Epoch 12/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5849 - auc: 0.7465 - val_loss: 0.5897 - val_auc: 0.7497\n",
      "Epoch 13/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5830 - auc: 0.7474 - val_loss: 0.5814 - val_auc: 0.7495\n",
      "Epoch 14/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5831 - auc: 0.7477 - val_loss: 0.5796 - val_auc: 0.7501\n",
      "Epoch 15/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5822 - auc: 0.7479 - val_loss: 0.5969 - val_auc: 0.7493\n",
      "Epoch 16/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5807 - auc: 0.7487 - val_loss: 0.5823 - val_auc: 0.7513\n",
      "Epoch 17/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5812 - auc: 0.7483 - val_loss: 0.5774 - val_auc: 0.7508\n",
      "Epoch 18/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5803 - auc: 0.7492 - val_loss: 0.5771 - val_auc: 0.7516\n",
      "Epoch 19/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5785 - auc: 0.7499 - val_loss: 0.5760 - val_auc: 0.7517\n",
      "Epoch 20/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5788 - auc: 0.7497 - val_loss: 0.5829 - val_auc: 0.7516\n",
      "Epoch 21/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5775 - auc: 0.7503 - val_loss: 0.5929 - val_auc: 0.7507\n",
      "Epoch 22/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5777 - auc: 0.7504 - val_loss: 0.5763 - val_auc: 0.7523\n",
      "Epoch 23/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5774 - auc: 0.7505 - val_loss: 0.5757 - val_auc: 0.7523\n",
      "Epoch 24/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5779 - auc: 0.7503 - val_loss: 0.5923 - val_auc: 0.7518\n",
      "Epoch 25/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5766 - auc: 0.7507 - val_loss: 0.5737 - val_auc: 0.7519\n",
      "Epoch 26/1000\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.5757 - auc: 0.7513 - val_loss: 0.5738 - val_auc: 0.7529\n",
      "Epoch 27/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5777 - auc: 0.7503 - val_loss: 0.5730 - val_auc: 0.7524\n",
      "Epoch 28/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5761 - auc: 0.7514 - val_loss: 0.5733 - val_auc: 0.7521\n",
      "Epoch 29/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5772 - auc: 0.7504 - val_loss: 0.5736 - val_auc: 0.7527\n",
      "Epoch 30/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5751 - auc: 0.7517 - val_loss: 0.5725 - val_auc: 0.7530\n",
      "Epoch 31/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5745 - auc: 0.7518 - val_loss: 0.5726 - val_auc: 0.7529\n",
      "Epoch 32/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5753 - auc: 0.7515 - val_loss: 0.5839 - val_auc: 0.7513\n",
      "Epoch 33/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5751 - auc: 0.7514 - val_loss: 0.5717 - val_auc: 0.7533\n",
      "Epoch 34/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5747 - auc: 0.7516 - val_loss: 0.5776 - val_auc: 0.7536\n",
      "Epoch 35/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5749 - auc: 0.7515 - val_loss: 0.5711 - val_auc: 0.7533\n",
      "Epoch 36/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5750 - auc: 0.7516 - val_loss: 0.5733 - val_auc: 0.7533\n",
      "Epoch 37/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5749 - auc: 0.7515 - val_loss: 0.5723 - val_auc: 0.7539\n",
      "Epoch 38/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5749 - auc: 0.7520 - val_loss: 0.5717 - val_auc: 0.7528\n",
      "Epoch 39/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5748 - auc: 0.7521 - val_loss: 0.5742 - val_auc: 0.7537\n",
      "Epoch 40/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5731 - auc: 0.7525 - val_loss: 0.5795 - val_auc: 0.7532\n",
      "Epoch 41/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5714 - auc: 0.7538 - val_loss: 0.5704 - val_auc: 0.7539\n",
      "Epoch 42/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5712 - auc: 0.7540 - val_loss: 0.5718 - val_auc: 0.7538\n",
      "Epoch 43/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5710 - auc: 0.7541 - val_loss: 0.5701 - val_auc: 0.7534\n",
      "Epoch 44/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5708 - auc: 0.7539 - val_loss: 0.5702 - val_auc: 0.7536\n",
      "Epoch 45/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5715 - auc: 0.7538 - val_loss: 0.5697 - val_auc: 0.7536\n",
      "Epoch 46/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5710 - auc: 0.7541 - val_loss: 0.5743 - val_auc: 0.7539\n",
      "Epoch 47/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5711 - auc: 0.7539 - val_loss: 0.5711 - val_auc: 0.7537\n",
      "Epoch 48/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5711 - auc: 0.7538 - val_loss: 0.5695 - val_auc: 0.7536\n",
      "Epoch 49/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5709 - auc: 0.7542 - val_loss: 0.5705 - val_auc: 0.7541\n",
      "Epoch 50/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5708 - auc: 0.7542 - val_loss: 0.5712 - val_auc: 0.7538\n",
      "Epoch 51/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5709 - auc: 0.7541 - val_loss: 0.5692 - val_auc: 0.7542\n",
      "Epoch 52/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5708 - auc: 0.7540 - val_loss: 0.5699 - val_auc: 0.7540\n",
      "Epoch 53/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5709 - auc: 0.7540 - val_loss: 0.5701 - val_auc: 0.7539\n",
      "Epoch 54/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7543 - val_loss: 0.5700 - val_auc: 0.7537\n",
      "Epoch 55/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7542 - val_loss: 0.5693 - val_auc: 0.7542\n",
      "Epoch 56/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7541 - val_loss: 0.5732 - val_auc: 0.7539\n",
      "Epoch 57/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5699 - auc: 0.7548 - val_loss: 0.5689 - val_auc: 0.7543\n",
      "Epoch 58/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5699 - auc: 0.7548 - val_loss: 0.5688 - val_auc: 0.7543\n",
      "Epoch 59/1000\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.5698 - auc: 0.7549 - val_loss: 0.5690 - val_auc: 0.7543\n",
      "Epoch 60/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7548 - val_loss: 0.5688 - val_auc: 0.7541\n",
      "Epoch 61/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7548 - val_loss: 0.5691 - val_auc: 0.7540\n",
      "Epoch 62/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7548 - val_loss: 0.5688 - val_auc: 0.7541\n",
      "Epoch 63/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7549 - val_loss: 0.5690 - val_auc: 0.7544\n",
      "Epoch 64/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7549 - val_loss: 0.5688 - val_auc: 0.7543\n",
      "Epoch 65/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7548 - val_loss: 0.5687 - val_auc: 0.7542\n",
      "Epoch 66/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7548 - val_loss: 0.5693 - val_auc: 0.7540\n",
      "Epoch 67/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7549 - val_loss: 0.5688 - val_auc: 0.7543\n",
      "Epoch 68/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5696 - auc: 0.7550 - val_loss: 0.5687 - val_auc: 0.7543\n",
      "Epoch 69/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5696 - auc: 0.7550 - val_loss: 0.5686 - val_auc: 0.7543\n",
      "Epoch 70/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5696 - auc: 0.7550 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 71/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5696 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7543\n",
      "Epoch 72/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5696 - auc: 0.7550 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 73/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5696 - auc: 0.7550 - val_loss: 0.5686 - val_auc: 0.7543\n",
      "Epoch 74/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7550 - val_loss: 0.5687 - val_auc: 0.7542\n",
      "Epoch 75/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 76/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 77/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7550 - val_loss: 0.5686 - val_auc: 0.7543\n",
      "Epoch 78/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7550 - val_loss: 0.5686 - val_auc: 0.7543\n",
      "Epoch 79/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7543\n",
      "Epoch 80/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7543\n",
      "Epoch 81/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 82/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7543\n",
      "Epoch 83/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 84/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 85/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 86/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 87/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 88/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 89/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 90/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 91/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 92/1000\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 93/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 94/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 95/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 96/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 97/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 98/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 99/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Epoch 100/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7551 - val_loss: 0.5686 - val_auc: 0.7544\n",
      "Fold 1 NN: 0.75439\n",
      "Training fold 2\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.6291 - auc: 0.7169 - val_loss: 0.6148 - val_auc: 0.7353\n",
      "Epoch 2/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6159 - auc: 0.7329 - val_loss: 0.6171 - val_auc: 0.7389\n",
      "Epoch 3/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6133 - auc: 0.7372 - val_loss: 0.6114 - val_auc: 0.7412\n",
      "Epoch 4/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6126 - auc: 0.7386 - val_loss: 0.6108 - val_auc: 0.7426\n",
      "Epoch 5/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6125 - auc: 0.7384 - val_loss: 0.6119 - val_auc: 0.7431\n",
      "Epoch 6/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6119 - auc: 0.7392 - val_loss: 0.6090 - val_auc: 0.7439\n",
      "Epoch 7/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6100 - auc: 0.7418 - val_loss: 0.6079 - val_auc: 0.7446\n",
      "Epoch 8/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.6088 - auc: 0.7410 - val_loss: 0.6036 - val_auc: 0.7447\n",
      "Epoch 9/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6000 - auc: 0.7410 - val_loss: 0.5931 - val_auc: 0.7445\n",
      "Epoch 10/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5934 - auc: 0.7418 - val_loss: 0.5897 - val_auc: 0.7448\n",
      "Epoch 11/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5897 - auc: 0.7441 - val_loss: 0.5869 - val_auc: 0.7456\n",
      "Epoch 12/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5871 - auc: 0.7458 - val_loss: 0.5847 - val_auc: 0.7468\n",
      "Epoch 13/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5860 - auc: 0.7467 - val_loss: 0.5895 - val_auc: 0.7461\n",
      "Epoch 14/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5856 - auc: 0.7468 - val_loss: 0.5843 - val_auc: 0.7481\n",
      "Epoch 15/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5847 - auc: 0.7469 - val_loss: 0.5848 - val_auc: 0.7484\n",
      "Epoch 16/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5841 - auc: 0.7477 - val_loss: 0.5819 - val_auc: 0.7487\n",
      "Epoch 17/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5832 - auc: 0.7483 - val_loss: 0.5808 - val_auc: 0.7490\n",
      "Epoch 18/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5818 - auc: 0.7491 - val_loss: 0.5796 - val_auc: 0.7496\n",
      "Epoch 19/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5815 - auc: 0.7490 - val_loss: 0.5930 - val_auc: 0.7495\n",
      "Epoch 20/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5811 - auc: 0.7491 - val_loss: 0.5788 - val_auc: 0.7497\n",
      "Epoch 21/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5801 - auc: 0.7497 - val_loss: 0.5781 - val_auc: 0.7499\n",
      "Epoch 22/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5798 - auc: 0.7498 - val_loss: 0.5784 - val_auc: 0.7506\n",
      "Epoch 23/1000\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.5790 - auc: 0.7506 - val_loss: 0.5783 - val_auc: 0.7507\n",
      "Epoch 24/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5784 - auc: 0.7507 - val_loss: 0.5771 - val_auc: 0.7504\n",
      "Epoch 25/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5786 - auc: 0.7506 - val_loss: 0.5800 - val_auc: 0.7496\n",
      "Epoch 26/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5779 - auc: 0.7509 - val_loss: 0.5776 - val_auc: 0.7510\n",
      "Epoch 27/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5779 - auc: 0.7507 - val_loss: 0.5759 - val_auc: 0.7503\n",
      "Epoch 28/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5773 - auc: 0.7511 - val_loss: 0.5824 - val_auc: 0.7510\n",
      "Epoch 29/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5776 - auc: 0.7510 - val_loss: 0.5802 - val_auc: 0.7510\n",
      "Epoch 30/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5766 - auc: 0.7516 - val_loss: 0.5745 - val_auc: 0.7512\n",
      "Epoch 31/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5768 - auc: 0.7514 - val_loss: 0.5749 - val_auc: 0.7520\n",
      "Epoch 32/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5766 - auc: 0.7512 - val_loss: 0.5741 - val_auc: 0.7517\n",
      "Epoch 33/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5750 - auc: 0.7520 - val_loss: 0.5746 - val_auc: 0.7520\n",
      "Epoch 34/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5767 - auc: 0.7513 - val_loss: 0.5765 - val_auc: 0.7520\n",
      "Epoch 35/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5757 - auc: 0.7515 - val_loss: 0.5802 - val_auc: 0.7519\n",
      "Epoch 36/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5758 - auc: 0.7520 - val_loss: 0.5738 - val_auc: 0.7523\n",
      "Epoch 37/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5756 - auc: 0.7519 - val_loss: 0.5771 - val_auc: 0.7524\n",
      "Epoch 38/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5752 - auc: 0.7523 - val_loss: 0.5835 - val_auc: 0.7524\n",
      "Epoch 39/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5758 - auc: 0.7515 - val_loss: 0.5747 - val_auc: 0.7520\n",
      "Epoch 40/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5742 - auc: 0.7524 - val_loss: 0.5737 - val_auc: 0.7518\n",
      "Epoch 41/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5737 - auc: 0.7527 - val_loss: 0.5739 - val_auc: 0.7521\n",
      "Epoch 42/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5724 - auc: 0.7538 - val_loss: 0.5730 - val_auc: 0.7524\n",
      "Epoch 43/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5724 - auc: 0.7539 - val_loss: 0.5726 - val_auc: 0.7525\n",
      "Epoch 44/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5724 - auc: 0.7538 - val_loss: 0.5719 - val_auc: 0.7524\n",
      "Epoch 45/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5722 - auc: 0.7540 - val_loss: 0.5718 - val_auc: 0.7524\n",
      "Epoch 46/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5722 - auc: 0.7539 - val_loss: 0.5722 - val_auc: 0.7521\n",
      "Epoch 47/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5720 - auc: 0.7541 - val_loss: 0.5721 - val_auc: 0.7525\n",
      "Epoch 48/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5718 - auc: 0.7541 - val_loss: 0.5728 - val_auc: 0.7524\n",
      "Epoch 49/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5719 - auc: 0.7541 - val_loss: 0.5717 - val_auc: 0.7527\n",
      "Epoch 50/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5721 - auc: 0.7538 - val_loss: 0.5726 - val_auc: 0.7523\n",
      "Epoch 51/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5718 - auc: 0.7541 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 52/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5716 - auc: 0.7541 - val_loss: 0.5724 - val_auc: 0.7527\n",
      "Epoch 53/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5717 - auc: 0.7545 - val_loss: 0.5713 - val_auc: 0.7528\n",
      "Epoch 54/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5718 - auc: 0.7542 - val_loss: 0.5724 - val_auc: 0.7523\n",
      "Epoch 55/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5715 - auc: 0.7542 - val_loss: 0.5715 - val_auc: 0.7529\n",
      "Epoch 56/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5717 - auc: 0.7543 - val_loss: 0.5717 - val_auc: 0.7529\n",
      "Epoch 57/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5716 - auc: 0.7542 - val_loss: 0.5712 - val_auc: 0.7528\n",
      "Epoch 58/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5715 - auc: 0.7544 - val_loss: 0.5721 - val_auc: 0.7529\n",
      "Epoch 59/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5708 - auc: 0.7549 - val_loss: 0.5711 - val_auc: 0.7528\n",
      "Epoch 60/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5708 - auc: 0.7549 - val_loss: 0.5711 - val_auc: 0.7528\n",
      "Epoch 61/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5708 - auc: 0.7548 - val_loss: 0.5713 - val_auc: 0.7527\n",
      "Epoch 62/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5709 - auc: 0.7547 - val_loss: 0.5713 - val_auc: 0.7526\n",
      "Epoch 63/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5708 - auc: 0.7548 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 64/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7550 - val_loss: 0.5713 - val_auc: 0.7529\n",
      "Epoch 65/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5708 - auc: 0.7549 - val_loss: 0.5713 - val_auc: 0.7524\n",
      "Epoch 66/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5708 - auc: 0.7549 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 67/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 68/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 69/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7549 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 70/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7527\n",
      "Epoch 71/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7527\n",
      "Epoch 72/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7527\n",
      "Epoch 73/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5710 - val_auc: 0.7528\n",
      "Epoch 74/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7527\n",
      "Epoch 75/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 76/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 77/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7527\n",
      "Epoch 78/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 79/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 80/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 81/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 82/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 83/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 84/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 85/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 86/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 87/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 88/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 89/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 90/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 91/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7550 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 92/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 93/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 94/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 95/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 96/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 97/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 98/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 99/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 100/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 101/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 102/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Epoch 103/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5705 - auc: 0.7551 - val_loss: 0.5709 - val_auc: 0.7528\n",
      "Fold 2 NN: 0.7528\n",
      "Training fold 3\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 0.6309 - auc: 0.7130 - val_loss: 0.6299 - val_auc: 0.7313\n",
      "Epoch 2/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6157 - auc: 0.7330 - val_loss: 0.6160 - val_auc: 0.7361\n",
      "Epoch 3/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6126 - auc: 0.7366 - val_loss: 0.6111 - val_auc: 0.7385\n",
      "Epoch 4/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6038 - auc: 0.7382 - val_loss: 0.5994 - val_auc: 0.7394\n",
      "Epoch 5/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5964 - auc: 0.7398 - val_loss: 0.5954 - val_auc: 0.7402\n",
      "Epoch 6/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5943 - auc: 0.7410 - val_loss: 0.5932 - val_auc: 0.7414\n",
      "Epoch 7/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5910 - auc: 0.7433 - val_loss: 0.5950 - val_auc: 0.7421\n",
      "Epoch 8/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5896 - auc: 0.7440 - val_loss: 0.5930 - val_auc: 0.7439\n",
      "Epoch 9/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5887 - auc: 0.7447 - val_loss: 0.5876 - val_auc: 0.7444\n",
      "Epoch 10/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5869 - auc: 0.7456 - val_loss: 0.5864 - val_auc: 0.7453\n",
      "Epoch 11/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5858 - auc: 0.7464 - val_loss: 0.5855 - val_auc: 0.7458\n",
      "Epoch 12/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5851 - auc: 0.7467 - val_loss: 0.5854 - val_auc: 0.7464\n",
      "Epoch 13/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5842 - auc: 0.7472 - val_loss: 0.5853 - val_auc: 0.7465\n",
      "Epoch 14/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5836 - auc: 0.7475 - val_loss: 0.5831 - val_auc: 0.7466\n",
      "Epoch 15/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5823 - auc: 0.7486 - val_loss: 0.5832 - val_auc: 0.7475\n",
      "Epoch 16/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5820 - auc: 0.7485 - val_loss: 0.5853 - val_auc: 0.7481\n",
      "Epoch 17/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5813 - auc: 0.7491 - val_loss: 0.5815 - val_auc: 0.7481\n",
      "Epoch 18/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5799 - auc: 0.7497 - val_loss: 0.5810 - val_auc: 0.7488\n",
      "Epoch 19/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5810 - auc: 0.7491 - val_loss: 0.5809 - val_auc: 0.7488\n",
      "Epoch 20/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5799 - auc: 0.7496 - val_loss: 0.5800 - val_auc: 0.7487\n",
      "Epoch 21/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5800 - auc: 0.7496 - val_loss: 0.5809 - val_auc: 0.7490\n",
      "Epoch 22/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5790 - auc: 0.7502 - val_loss: 0.5842 - val_auc: 0.7493\n",
      "Epoch 23/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5786 - auc: 0.7501 - val_loss: 0.5800 - val_auc: 0.7495\n",
      "Epoch 24/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5789 - auc: 0.7503 - val_loss: 0.5785 - val_auc: 0.7494\n",
      "Epoch 25/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5776 - auc: 0.7506 - val_loss: 0.5806 - val_auc: 0.7497\n",
      "Epoch 26/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5772 - auc: 0.7511 - val_loss: 0.5778 - val_auc: 0.7497\n",
      "Epoch 27/1000\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.5770 - auc: 0.7510 - val_loss: 0.5877 - val_auc: 0.7491\n",
      "Epoch 28/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5769 - auc: 0.7510 - val_loss: 0.5789 - val_auc: 0.7501\n",
      "Epoch 29/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5764 - auc: 0.7512 - val_loss: 0.5768 - val_auc: 0.7504\n",
      "Epoch 30/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5760 - auc: 0.7514 - val_loss: 0.5762 - val_auc: 0.7507\n",
      "Epoch 31/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5757 - auc: 0.7514 - val_loss: 0.5779 - val_auc: 0.7504\n",
      "Epoch 32/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5750 - auc: 0.7517 - val_loss: 0.5763 - val_auc: 0.7508\n",
      "Epoch 33/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5764 - auc: 0.7516 - val_loss: 0.5844 - val_auc: 0.7505\n",
      "Epoch 34/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5760 - auc: 0.7514 - val_loss: 0.5792 - val_auc: 0.7507\n",
      "Epoch 35/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5756 - auc: 0.7519 - val_loss: 0.5754 - val_auc: 0.7509\n",
      "Epoch 36/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5754 - auc: 0.7519 - val_loss: 0.5755 - val_auc: 0.7511\n",
      "Epoch 37/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5747 - auc: 0.7522 - val_loss: 0.5752 - val_auc: 0.7513\n",
      "Epoch 38/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5740 - auc: 0.7525 - val_loss: 0.5777 - val_auc: 0.7514\n",
      "Epoch 39/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5746 - auc: 0.7522 - val_loss: 0.5761 - val_auc: 0.7509\n",
      "Epoch 40/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5746 - auc: 0.7522 - val_loss: 0.5741 - val_auc: 0.7516\n",
      "Epoch 41/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5744 - auc: 0.7522 - val_loss: 0.5811 - val_auc: 0.7509\n",
      "Epoch 42/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5743 - auc: 0.7525 - val_loss: 0.5751 - val_auc: 0.7509\n",
      "Epoch 43/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5736 - auc: 0.7527 - val_loss: 0.5746 - val_auc: 0.7512\n",
      "Epoch 44/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5734 - auc: 0.7529 - val_loss: 0.5765 - val_auc: 0.7513\n",
      "Epoch 45/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5730 - auc: 0.7531 - val_loss: 0.5738 - val_auc: 0.7514\n",
      "Epoch 46/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5736 - auc: 0.7527 - val_loss: 0.5738 - val_auc: 0.7515\n",
      "Epoch 47/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5733 - auc: 0.7531 - val_loss: 0.5801 - val_auc: 0.7514\n",
      "Epoch 48/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5738 - auc: 0.7526 - val_loss: 0.5736 - val_auc: 0.7517\n",
      "Epoch 49/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5731 - auc: 0.7531 - val_loss: 0.5735 - val_auc: 0.7511\n",
      "Epoch 50/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5734 - auc: 0.7530 - val_loss: 0.5969 - val_auc: 0.7513\n",
      "Epoch 51/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5735 - auc: 0.7529 - val_loss: 0.5784 - val_auc: 0.7513\n",
      "Epoch 52/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5736 - auc: 0.7527 - val_loss: 0.5731 - val_auc: 0.7514\n",
      "Epoch 53/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5729 - auc: 0.7535 - val_loss: 0.5738 - val_auc: 0.7514\n",
      "Epoch 54/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5725 - auc: 0.7534 - val_loss: 0.5753 - val_auc: 0.7517\n",
      "Epoch 55/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5727 - auc: 0.7532 - val_loss: 0.5728 - val_auc: 0.7513\n",
      "Epoch 56/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5734 - auc: 0.7530 - val_loss: 0.5766 - val_auc: 0.7515\n",
      "Epoch 57/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5725 - auc: 0.7538 - val_loss: 0.5752 - val_auc: 0.7517\n",
      "Epoch 58/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5730 - auc: 0.7532 - val_loss: 0.5733 - val_auc: 0.7513\n",
      "Epoch 59/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5720 - auc: 0.7539 - val_loss: 0.5728 - val_auc: 0.7518\n",
      "Epoch 60/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5723 - auc: 0.7536 - val_loss: 0.5742 - val_auc: 0.7510\n",
      "Epoch 61/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5703 - auc: 0.7551 - val_loss: 0.5721 - val_auc: 0.7517\n",
      "Epoch 62/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5702 - auc: 0.7552 - val_loss: 0.5721 - val_auc: 0.7517\n",
      "Epoch 63/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5699 - auc: 0.7553 - val_loss: 0.5722 - val_auc: 0.7519\n",
      "Epoch 64/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5700 - auc: 0.7552 - val_loss: 0.5725 - val_auc: 0.7518\n",
      "Epoch 65/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7552 - val_loss: 0.5734 - val_auc: 0.7518\n",
      "Epoch 66/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5702 - auc: 0.7553 - val_loss: 0.5721 - val_auc: 0.7518\n",
      "Epoch 67/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5695 - auc: 0.7557 - val_loss: 0.5720 - val_auc: 0.7518\n",
      "Epoch 68/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5694 - auc: 0.7557 - val_loss: 0.5720 - val_auc: 0.7519\n",
      "Epoch 69/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5694 - auc: 0.7557 - val_loss: 0.5723 - val_auc: 0.7518\n",
      "Epoch 70/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5694 - auc: 0.7557 - val_loss: 0.5722 - val_auc: 0.7519\n",
      "Epoch 71/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5694 - auc: 0.7557 - val_loss: 0.5719 - val_auc: 0.7520\n",
      "Epoch 72/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5694 - auc: 0.7556 - val_loss: 0.5722 - val_auc: 0.7519\n",
      "Epoch 73/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5693 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 74/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 75/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5693 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 76/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5693 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 77/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 78/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 79/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 80/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 81/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 82/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 83/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 84/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 85/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 86/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 87/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 88/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 89/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 90/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 91/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 92/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 93/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 94/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 95/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 96/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 97/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 98/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 99/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 100/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 101/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 102/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 103/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 104/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 105/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 106/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 107/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Epoch 108/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5692 - auc: 0.7558 - val_loss: 0.5719 - val_auc: 0.7518\n",
      "Fold 3 NN: 0.75187\n",
      "Training fold 4\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "235/235 [==============================] - 2s 5ms/step - loss: 0.6311 - auc: 0.7133 - val_loss: 0.6172 - val_auc: 0.7313\n",
      "Epoch 2/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6168 - auc: 0.7313 - val_loss: 0.6249 - val_auc: 0.7358\n",
      "Epoch 3/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6143 - auc: 0.7352 - val_loss: 0.6124 - val_auc: 0.7384\n",
      "Epoch 4/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6120 - auc: 0.7385 - val_loss: 0.6108 - val_auc: 0.7404\n",
      "Epoch 5/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.6105 - auc: 0.7394 - val_loss: 0.6094 - val_auc: 0.7415\n",
      "Epoch 6/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6022 - auc: 0.7397 - val_loss: 0.5942 - val_auc: 0.7418\n",
      "Epoch 7/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5934 - auc: 0.7417 - val_loss: 0.6044 - val_auc: 0.7429\n",
      "Epoch 8/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5920 - auc: 0.7422 - val_loss: 0.5900 - val_auc: 0.7444\n",
      "Epoch 9/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5882 - auc: 0.7445 - val_loss: 0.5873 - val_auc: 0.7459\n",
      "Epoch 10/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5875 - auc: 0.7452 - val_loss: 0.5874 - val_auc: 0.7465\n",
      "Epoch 11/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5862 - auc: 0.7456 - val_loss: 0.5894 - val_auc: 0.7467\n",
      "Epoch 12/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5849 - auc: 0.7465 - val_loss: 0.5844 - val_auc: 0.7479\n",
      "Epoch 13/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5839 - auc: 0.7472 - val_loss: 0.5850 - val_auc: 0.7476\n",
      "Epoch 14/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5843 - auc: 0.7470 - val_loss: 0.5823 - val_auc: 0.7482\n",
      "Epoch 15/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5826 - auc: 0.7478 - val_loss: 0.5906 - val_auc: 0.7478\n",
      "Epoch 16/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5828 - auc: 0.7479 - val_loss: 0.5813 - val_auc: 0.7488\n",
      "Epoch 17/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5810 - auc: 0.7489 - val_loss: 0.5803 - val_auc: 0.7497\n",
      "Epoch 18/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5810 - auc: 0.7488 - val_loss: 0.5801 - val_auc: 0.7490\n",
      "Epoch 19/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5809 - auc: 0.7488 - val_loss: 0.5789 - val_auc: 0.7499\n",
      "Epoch 20/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5799 - auc: 0.7497 - val_loss: 0.5784 - val_auc: 0.7498\n",
      "Epoch 21/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5791 - auc: 0.7502 - val_loss: 0.5781 - val_auc: 0.7498\n",
      "Epoch 22/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5787 - auc: 0.7502 - val_loss: 0.5778 - val_auc: 0.7504\n",
      "Epoch 23/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5788 - auc: 0.7501 - val_loss: 0.5784 - val_auc: 0.7505\n",
      "Epoch 24/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5779 - auc: 0.7507 - val_loss: 0.5774 - val_auc: 0.7506\n",
      "Epoch 25/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5777 - auc: 0.7507 - val_loss: 0.5879 - val_auc: 0.7501\n",
      "Epoch 26/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5789 - auc: 0.7498 - val_loss: 0.5764 - val_auc: 0.7511\n",
      "Epoch 27/1000\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.5771 - auc: 0.7510 - val_loss: 0.5766 - val_auc: 0.7502\n",
      "Epoch 28/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5765 - auc: 0.7514 - val_loss: 0.5889 - val_auc: 0.7501\n",
      "Epoch 29/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5766 - auc: 0.7511 - val_loss: 0.5866 - val_auc: 0.7508\n",
      "Epoch 30/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5769 - auc: 0.7512 - val_loss: 0.5813 - val_auc: 0.7501\n",
      "Epoch 31/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5764 - auc: 0.7514 - val_loss: 0.5763 - val_auc: 0.7511\n",
      "Epoch 32/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5764 - auc: 0.7515 - val_loss: 0.5757 - val_auc: 0.7512\n",
      "Epoch 33/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5749 - auc: 0.7520 - val_loss: 0.5844 - val_auc: 0.7517\n",
      "Epoch 34/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5767 - auc: 0.7515 - val_loss: 0.5771 - val_auc: 0.7500\n",
      "Epoch 35/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5759 - auc: 0.7517 - val_loss: 0.5744 - val_auc: 0.7511\n",
      "Epoch 36/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5743 - auc: 0.7524 - val_loss: 0.5785 - val_auc: 0.7512\n",
      "Epoch 37/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5753 - auc: 0.7519 - val_loss: 0.5836 - val_auc: 0.7525\n",
      "Epoch 38/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5755 - auc: 0.7519 - val_loss: 0.5736 - val_auc: 0.7513\n",
      "Epoch 39/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5743 - auc: 0.7525 - val_loss: 0.5736 - val_auc: 0.7509\n",
      "Epoch 40/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5751 - auc: 0.7522 - val_loss: 0.5731 - val_auc: 0.7514\n",
      "Epoch 41/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5739 - auc: 0.7524 - val_loss: 0.5768 - val_auc: 0.7519\n",
      "Epoch 42/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5737 - auc: 0.7531 - val_loss: 0.5782 - val_auc: 0.7517\n",
      "Epoch 43/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5734 - auc: 0.7531 - val_loss: 0.5734 - val_auc: 0.7510\n",
      "Epoch 44/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5734 - auc: 0.7530 - val_loss: 0.5745 - val_auc: 0.7521\n",
      "Epoch 45/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5736 - auc: 0.7529 - val_loss: 0.5786 - val_auc: 0.7518\n",
      "Epoch 46/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5714 - auc: 0.7543 - val_loss: 0.5721 - val_auc: 0.7518\n",
      "Epoch 47/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5712 - auc: 0.7544 - val_loss: 0.5731 - val_auc: 0.7518\n",
      "Epoch 48/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5714 - auc: 0.7544 - val_loss: 0.5726 - val_auc: 0.7519\n",
      "Epoch 49/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5713 - auc: 0.7547 - val_loss: 0.5721 - val_auc: 0.7519\n",
      "Epoch 50/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5714 - auc: 0.7547 - val_loss: 0.5719 - val_auc: 0.7520\n",
      "Epoch 51/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5712 - auc: 0.7545 - val_loss: 0.5719 - val_auc: 0.7519\n",
      "Epoch 52/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5712 - auc: 0.7546 - val_loss: 0.5718 - val_auc: 0.7523\n",
      "Epoch 53/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5710 - auc: 0.7546 - val_loss: 0.5719 - val_auc: 0.7523\n",
      "Epoch 54/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5708 - auc: 0.7546 - val_loss: 0.5723 - val_auc: 0.7522\n",
      "Epoch 55/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5710 - auc: 0.7547 - val_loss: 0.5719 - val_auc: 0.7524\n",
      "Epoch 56/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5710 - auc: 0.7548 - val_loss: 0.5717 - val_auc: 0.7525\n",
      "Epoch 57/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5713 - auc: 0.7548 - val_loss: 0.5726 - val_auc: 0.7521\n",
      "Epoch 58/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5712 - auc: 0.7546 - val_loss: 0.5724 - val_auc: 0.7521\n",
      "Epoch 59/1000\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.5708 - auc: 0.7548 - val_loss: 0.5740 - val_auc: 0.7521\n",
      "Epoch 60/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5716 - val_auc: 0.7523\n",
      "Epoch 61/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5720 - val_auc: 0.7525\n",
      "Epoch 62/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5701 - auc: 0.7553 - val_loss: 0.5716 - val_auc: 0.7524\n",
      "Epoch 63/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5700 - auc: 0.7554 - val_loss: 0.5712 - val_auc: 0.7524\n",
      "Epoch 64/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5700 - auc: 0.7554 - val_loss: 0.5712 - val_auc: 0.7525\n",
      "Epoch 65/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5700 - auc: 0.7553 - val_loss: 0.5712 - val_auc: 0.7524\n",
      "Epoch 66/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5700 - auc: 0.7554 - val_loss: 0.5715 - val_auc: 0.7524\n",
      "Epoch 67/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5700 - auc: 0.7555 - val_loss: 0.5721 - val_auc: 0.7526\n",
      "Epoch 68/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5699 - auc: 0.7554 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 69/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 70/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5698 - auc: 0.7555 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 71/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7555 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 72/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7555 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 73/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5698 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 74/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5698 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 75/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5698 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 76/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5698 - auc: 0.7555 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 77/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 78/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 79/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 80/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5698 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 81/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 82/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 83/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5698 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 84/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 85/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 86/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 87/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 88/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 89/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 90/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7525\n",
      "Epoch 91/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 92/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 93/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 94/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 95/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 96/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Epoch 97/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5697 - auc: 0.7556 - val_loss: 0.5711 - val_auc: 0.7524\n",
      "Fold 4 NN: 0.75247\n",
      "Training fold 5\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.6269 - auc: 0.7189 - val_loss: 0.6182 - val_auc: 0.7318\n",
      "Epoch 2/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6152 - auc: 0.7336 - val_loss: 0.6151 - val_auc: 0.7364\n",
      "Epoch 3/1000\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.6122 - auc: 0.7374 - val_loss: 0.6116 - val_auc: 0.7391\n",
      "Epoch 4/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.6078 - auc: 0.7390 - val_loss: 0.6023 - val_auc: 0.7400\n",
      "Epoch 5/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5979 - auc: 0.7398 - val_loss: 0.5938 - val_auc: 0.7420\n",
      "Epoch 6/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5936 - auc: 0.7418 - val_loss: 0.5909 - val_auc: 0.7435\n",
      "Epoch 7/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5911 - auc: 0.7433 - val_loss: 0.5888 - val_auc: 0.7449\n",
      "Epoch 8/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5895 - auc: 0.7444 - val_loss: 0.5968 - val_auc: 0.7461\n",
      "Epoch 9/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5883 - auc: 0.7447 - val_loss: 0.5877 - val_auc: 0.7470\n",
      "Epoch 10/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5879 - auc: 0.7451 - val_loss: 0.5863 - val_auc: 0.7469\n",
      "Epoch 11/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5855 - auc: 0.7465 - val_loss: 0.5840 - val_auc: 0.7479\n",
      "Epoch 12/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5860 - auc: 0.7461 - val_loss: 0.5828 - val_auc: 0.7482\n",
      "Epoch 13/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5851 - auc: 0.7469 - val_loss: 0.5822 - val_auc: 0.7485\n",
      "Epoch 14/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5832 - auc: 0.7480 - val_loss: 0.5811 - val_auc: 0.7492\n",
      "Epoch 15/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5823 - auc: 0.7483 - val_loss: 0.5819 - val_auc: 0.7494\n",
      "Epoch 16/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5836 - auc: 0.7474 - val_loss: 0.5807 - val_auc: 0.7493\n",
      "Epoch 17/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5809 - auc: 0.7490 - val_loss: 0.5797 - val_auc: 0.7504\n",
      "Epoch 18/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5809 - auc: 0.7490 - val_loss: 0.5791 - val_auc: 0.7505\n",
      "Epoch 19/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5802 - auc: 0.7493 - val_loss: 0.5796 - val_auc: 0.7503\n",
      "Epoch 20/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5796 - auc: 0.7496 - val_loss: 0.5781 - val_auc: 0.7508\n",
      "Epoch 21/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5797 - auc: 0.7497 - val_loss: 0.5777 - val_auc: 0.7510\n",
      "Epoch 22/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5790 - auc: 0.7500 - val_loss: 0.5771 - val_auc: 0.7513\n",
      "Epoch 23/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5787 - auc: 0.7503 - val_loss: 0.5820 - val_auc: 0.7510\n",
      "Epoch 24/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5779 - auc: 0.7506 - val_loss: 0.5781 - val_auc: 0.7510\n",
      "Epoch 25/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5774 - auc: 0.7508 - val_loss: 0.5761 - val_auc: 0.7513\n",
      "Epoch 26/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5777 - auc: 0.7506 - val_loss: 0.5775 - val_auc: 0.7514\n",
      "Epoch 27/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5772 - auc: 0.7509 - val_loss: 0.5783 - val_auc: 0.7515\n",
      "Epoch 28/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5772 - auc: 0.7510 - val_loss: 0.5767 - val_auc: 0.7516\n",
      "Epoch 29/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5760 - auc: 0.7514 - val_loss: 0.5771 - val_auc: 0.7518\n",
      "Epoch 30/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5761 - auc: 0.7513 - val_loss: 0.5747 - val_auc: 0.7519\n",
      "Epoch 31/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5770 - auc: 0.7511 - val_loss: 0.5751 - val_auc: 0.7517\n",
      "Epoch 32/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5766 - auc: 0.7514 - val_loss: 0.5764 - val_auc: 0.7517\n",
      "Epoch 33/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5759 - auc: 0.7517 - val_loss: 0.5773 - val_auc: 0.7510\n",
      "Epoch 34/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5759 - auc: 0.7517 - val_loss: 0.5788 - val_auc: 0.7518\n",
      "Epoch 35/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5758 - auc: 0.7516 - val_loss: 0.5844 - val_auc: 0.7509\n",
      "Epoch 36/1000\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.5736 - auc: 0.7529 - val_loss: 0.5735 - val_auc: 0.7521\n",
      "Epoch 37/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5731 - auc: 0.7532 - val_loss: 0.5734 - val_auc: 0.7521\n",
      "Epoch 38/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5733 - auc: 0.7531 - val_loss: 0.5748 - val_auc: 0.7520\n",
      "Epoch 39/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5731 - auc: 0.7533 - val_loss: 0.5733 - val_auc: 0.7522\n",
      "Epoch 40/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5729 - auc: 0.7532 - val_loss: 0.5736 - val_auc: 0.7522\n",
      "Epoch 41/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5730 - auc: 0.7534 - val_loss: 0.5731 - val_auc: 0.7523\n",
      "Epoch 42/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5730 - auc: 0.7533 - val_loss: 0.5731 - val_auc: 0.7522\n",
      "Epoch 43/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5728 - auc: 0.7534 - val_loss: 0.5750 - val_auc: 0.7524\n",
      "Epoch 44/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5727 - auc: 0.7534 - val_loss: 0.5730 - val_auc: 0.7524\n",
      "Epoch 45/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5728 - auc: 0.7534 - val_loss: 0.5733 - val_auc: 0.7521\n",
      "Epoch 46/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5726 - auc: 0.7536 - val_loss: 0.5728 - val_auc: 0.7523\n",
      "Epoch 47/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5726 - auc: 0.7537 - val_loss: 0.5733 - val_auc: 0.7525\n",
      "Epoch 48/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5726 - auc: 0.7535 - val_loss: 0.5727 - val_auc: 0.7524\n",
      "Epoch 49/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5724 - auc: 0.7537 - val_loss: 0.5732 - val_auc: 0.7522\n",
      "Epoch 50/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5722 - auc: 0.7538 - val_loss: 0.5726 - val_auc: 0.7523\n",
      "Epoch 51/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5724 - auc: 0.7537 - val_loss: 0.5740 - val_auc: 0.7523\n",
      "Epoch 52/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5724 - auc: 0.7538 - val_loss: 0.5727 - val_auc: 0.7523\n",
      "Epoch 53/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5724 - auc: 0.7538 - val_loss: 0.5752 - val_auc: 0.7526\n",
      "Epoch 54/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5721 - auc: 0.7538 - val_loss: 0.5724 - val_auc: 0.7523\n",
      "Epoch 55/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5719 - auc: 0.7540 - val_loss: 0.5731 - val_auc: 0.7524\n",
      "Epoch 56/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5722 - auc: 0.7539 - val_loss: 0.5731 - val_auc: 0.7523\n",
      "Epoch 57/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5720 - auc: 0.7540 - val_loss: 0.5728 - val_auc: 0.7527\n",
      "Epoch 58/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5717 - auc: 0.7541 - val_loss: 0.5729 - val_auc: 0.7525\n",
      "Epoch 59/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5718 - auc: 0.7542 - val_loss: 0.5720 - val_auc: 0.7525\n",
      "Epoch 60/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5716 - auc: 0.7543 - val_loss: 0.5723 - val_auc: 0.7523\n",
      "Epoch 61/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5717 - auc: 0.7541 - val_loss: 0.5723 - val_auc: 0.7525\n",
      "Epoch 62/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5716 - auc: 0.7544 - val_loss: 0.5720 - val_auc: 0.7525\n",
      "Epoch 63/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5718 - auc: 0.7541 - val_loss: 0.5739 - val_auc: 0.7528\n",
      "Epoch 64/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5714 - auc: 0.7544 - val_loss: 0.5725 - val_auc: 0.7525\n",
      "Epoch 65/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5709 - auc: 0.7547 - val_loss: 0.5717 - val_auc: 0.7526\n",
      "Epoch 66/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5709 - auc: 0.7547 - val_loss: 0.5718 - val_auc: 0.7525\n",
      "Epoch 67/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5708 - auc: 0.7547 - val_loss: 0.5719 - val_auc: 0.7525\n",
      "Epoch 68/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5709 - auc: 0.7547 - val_loss: 0.5718 - val_auc: 0.7526\n",
      "Epoch 69/1000\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.5709 - auc: 0.7547 - val_loss: 0.5720 - val_auc: 0.7527\n",
      "Epoch 70/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5708 - auc: 0.7547 - val_loss: 0.5719 - val_auc: 0.7526\n",
      "Epoch 71/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5717 - val_auc: 0.7526\n",
      "Epoch 72/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5716 - val_auc: 0.7526\n",
      "Epoch 73/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5716 - val_auc: 0.7526\n",
      "Epoch 74/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 75/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5720 - val_auc: 0.7526\n",
      "Epoch 76/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5716 - val_auc: 0.7526\n",
      "Epoch 77/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 78/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 79/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5716 - val_auc: 0.7526\n",
      "Epoch 80/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7549 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 81/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 82/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 83/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 84/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5707 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 85/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 86/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 87/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 88/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 89/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 90/1000\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 91/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 92/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 93/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 94/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 95/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 96/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 97/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 98/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 99/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 100/1000\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 101/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 102/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Epoch 103/1000\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.5706 - auc: 0.7550 - val_loss: 0.5716 - val_auc: 0.7525\n",
      "Fold 5 NN: 0.7526\n"
     ]
    }
   ],
   "source": [
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "for fold, (trn_ind, val_ind) in enumerate(kf.split(train,train['target'])):\n",
    "    print(f'Training fold {fold + 1}')\n",
    "    X_train, X_test = train_nn.iloc[trn_ind][features_to_consider], train_nn.iloc[val_ind][features_to_consider]\n",
    "    y_train, y_test = train_nn.iloc[trn_ind]['target'], train_nn.iloc[val_ind]['target']\n",
    "    print('CV {}/{}'.format(counter, n_folds)) \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics = ['AUC']\n",
    "    )\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "      \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "\n",
    "    model.fit([num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([num_data_test]).reshape(1,-1)[0]\n",
    "    score = round(roc_auc_score(y_test,preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    \n",
    "    test_predictions_nn += model.predict([tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds       \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "888e37b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:32:40.805433Z",
     "iopub.status.busy": "2021-11-02T06:32:40.804787Z",
     "iopub.status.idle": "2021-11-02T06:32:40.980979Z",
     "shell.execute_reply": "2021-11-02T06:32:40.981468Z"
    },
    "papermill": {
     "duration": 2.801138,
     "end_time": "2021-11-02T06:32:40.981619",
     "exception": false,
     "start_time": "2021-11-02T06:32:38.180481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub=pd.read_csv(\"../input/tabular-playground-series-nov-2021/sample_submission.csv\")\n",
    "sub['target']=test_predictions_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0b9c727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:32:46.211179Z",
     "iopub.status.busy": "2021-11-02T06:32:46.210349Z",
     "iopub.status.idle": "2021-11-02T06:32:46.214809Z",
     "shell.execute_reply": "2021-11-02T06:32:46.215281Z"
    },
    "papermill": {
     "duration": 2.635242,
     "end_time": "2021-11-02T06:32:46.215485",
     "exception": false,
     "start_time": "2021-11-02T06:32:43.580243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>600000</td>\n",
       "      <td>0.735525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>600001</td>\n",
       "      <td>0.745809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>600002</td>\n",
       "      <td>0.755927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>600003</td>\n",
       "      <td>0.480474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>600004</td>\n",
       "      <td>0.723155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539995</th>\n",
       "      <td>1139995</td>\n",
       "      <td>0.754072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539996</th>\n",
       "      <td>1139996</td>\n",
       "      <td>0.734090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539997</th>\n",
       "      <td>1139997</td>\n",
       "      <td>0.634256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539998</th>\n",
       "      <td>1139998</td>\n",
       "      <td>0.730120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539999</th>\n",
       "      <td>1139999</td>\n",
       "      <td>0.752730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id    target\n",
       "0        600000  0.735525\n",
       "1        600001  0.745809\n",
       "2        600002  0.755927\n",
       "3        600003  0.480474\n",
       "4        600004  0.723155\n",
       "...         ...       ...\n",
       "539995  1139995  0.754072\n",
       "539996  1139996  0.734090\n",
       "539997  1139997  0.634256\n",
       "539998  1139998  0.730120\n",
       "539999  1139999  0.752730\n",
       "\n",
       "[540000 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06197533",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-02T06:32:51.701136Z",
     "iopub.status.busy": "2021-11-02T06:32:51.700346Z",
     "iopub.status.idle": "2021-11-02T06:32:53.376040Z",
     "shell.execute_reply": "2021-11-02T06:32:53.375127Z"
    },
    "papermill": {
     "duration": 4.246566,
     "end_time": "2021-11-02T06:32:53.376180",
     "exception": false,
     "start_time": "2021-11-02T06:32:49.129614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06c07e",
   "metadata": {
    "papermill": {
     "duration": 2.840181,
     "end_time": "2021-11-02T06:32:58.871361",
     "exception": false,
     "start_time": "2021-11-02T06:32:56.031180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 799.403306,
   "end_time": "2021-11-02T06:33:04.931099",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-02T06:19:45.527793",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
